[
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "weekly-notes/index.html",
    "href": "weekly-notes/index.html",
    "title": "Weekly Notes",
    "section": "",
    "text": "Welcome to my weekly notes for MUSA 5080 - Public Policy Analytics. Here you’ll find my reflections, key concepts, and learning outcomes from each week.\n\n\n\nWeek 1 Notes\nWeek 2 Notes\n\nMore notes will be added as the course progresses."
  },
  {
    "objectID": "weekly-notes/index.html#available-notes",
    "href": "weekly-notes/index.html#available-notes",
    "title": "Weekly Notes",
    "section": "",
    "text": "Week 1 Notes\nWeek 2 Notes\n\nMore notes will be added as the course progresses."
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html",
    "href": "assignments/assignment_4/assignment4.html",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "",
    "text": "Graffiti represents a persistent challenge to urban neighborhoods, affecting aesthetic quality, public safety perceptions, and property values. Chicago’s 311 service request system provides a comprehensive record of graffiti removal requests across the city. This analysis applies spatial predictive modeling to understand whether we can forecast graffiti concentrations using spatial features and count regression models.\nAnalysis Goals: - Develop a spatial predictive model using k-nearest neighbor and Local Moran’s I features - Compare Poisson and Negative Binomial regression performance - Validate models using spatial cross-validation (Leave-One-Group-Out) - Test temporal stability using 2018 crime data - Evaluate predictive accuracy against kernel density estimation baseline"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#loading-required-libraries",
    "href": "assignments/assignment_4/assignment4.html#loading-required-libraries",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Loading Required Libraries",
    "text": "Loading Required Libraries\n\n\nCode\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(sp)\nlibrary(raster)\nlibrary(spdep)\nlibrary(spatstat)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(viridis)\nlibrary(MASS)\nlibrary(broom)\nlibrary(jsonlite)\nlibrary(httr)\n\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#downloading-graffiti-data-from-chicago-311-portal",
    "href": "assignments/assignment_4/assignment4.html#downloading-graffiti-data-from-chicago-311-portal",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Downloading Graffiti Data from Chicago 311 Portal",
    "text": "Downloading Graffiti Data from Chicago 311 Portal\n\n\nCode\n# Chicago 311 API endpoint for graffiti data\napi_url &lt;- \"https://data.cityofchicago.org/resource/hec5-y4x5.json\"\n\n# Query parameters: download all available data\nparams &lt;- list(\n  \"$limit\" = 50000\n)\n\n# Execute API request\nresponse &lt;- GET(api_url, query = params)\ngraffiti_raw &lt;- fromJSON(content(response, \"text\"))\ngraffiti_df &lt;- as_tibble(graffiti_raw)\n\ncat(\"Raw Graffiti Data Summary:\\n\")\n\n\nRaw Graffiti Data Summary:\n\n\nCode\ncat(\"Total Records:\", nrow(graffiti_df), \"\\n\")\n\n\nTotal Records: 50000 \n\n\nCode\ncat(\"Variables:\", ncol(graffiti_df), \"\\n\\n\")\n\n\nVariables: 23 \n\n\nCode\n# Check available columns\ncat(\"Available columns:\\n\")\n\n\nAvailable columns:\n\n\nCode\nprint(colnames(graffiti_df))\n\n\n [1] \"creation_date\"                           \n [2] \"status\"                                  \n [3] \"completion_date\"                         \n [4] \"service_request_number\"                  \n [5] \"type_of_service_request\"                 \n [6] \"what_type_of_surface_is_the_graffiti_on_\"\n [7] \"where_is_the_graffiti_located_\"          \n [8] \"street_address\"                          \n [9] \"zip_code\"                                \n[10] \"x_coordinate\"                            \n[11] \"y_coordinate\"                            \n[12] \"ward\"                                    \n[13] \"police_district\"                         \n[14] \"community_area\"                          \n[15] \"latitude\"                                \n[16] \"longitude\"                               \n[17] \"location\"                                \n[18] \":@computed_region_awaf_s7ux\"             \n[19] \":@computed_region_6mkv_f3dw\"             \n[20] \":@computed_region_vrxf_vc4k\"             \n[21] \":@computed_region_bdys_3d7i\"             \n[22] \":@computed_region_43wa_7qmu\"             \n[23] \"ssa\""
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#data-cleaning-and-preparation",
    "href": "assignments/assignment_4/assignment4.html#data-cleaning-and-preparation",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Data Cleaning and Preparation",
    "text": "Data Cleaning and Preparation\n\n\nCode\n# Clean and prepare graffiti data\ngraffiti_clean &lt;- graffiti_df %&gt;%\n  filter(!is.na(longitude) & !is.na(latitude)) %&gt;%\n  mutate(\n    longitude = as.numeric(longitude),\n    latitude = as.numeric(latitude),\n    creation_date = as.POSIXct(creation_date),\n    year = year(creation_date),\n    month = month(creation_date),\n    date = as_date(creation_date)\n  ) %&gt;%\n  filter(longitude &gt; -88.5 & longitude &lt; -87.5,\n         latitude &gt; 41.6 & latitude &lt; 42.1)\n\ncat(\"Data Cleaning Summary:\\n\")\n\n\nData Cleaning Summary:\n\n\nCode\ncat(\"Records after cleaning: \", nrow(graffiti_clean), \"\\n\")\n\n\nRecords after cleaning:  49973 \n\n\nCode\nif(nrow(graffiti_clean) &gt; 0) {\n  cat(\"Date range: \", min(graffiti_clean$date, na.rm = TRUE), \n      \" to \", max(graffiti_clean$date, na.rm = TRUE), \"\\n\")\n}\n\n\nDate range:  17709  to  17883 \n\n\nCode\nhead(graffiti_clean, 5)\n\n\n# A tibble: 5 × 26\n  creation_date       status    completion_date         service_request_number\n  &lt;dttm&gt;              &lt;chr&gt;     &lt;chr&gt;                   &lt;chr&gt;                 \n1 2018-12-18 00:00:00 Completed 2018-12-19T00:00:00.000 18-03388768           \n2 2018-12-18 00:00:00 Completed 2018-12-19T00:00:00.000 18-03388476           \n3 2018-12-18 00:00:00 Open      &lt;NA&gt;                    18-03386436           \n4 2018-12-18 00:00:00 Completed 2018-12-19T00:00:00.000 18-03386535           \n5 2018-12-18 00:00:00 Open      &lt;NA&gt;                    18-03388464           \n# ℹ 22 more variables: type_of_service_request &lt;chr&gt;,\n#   what_type_of_surface_is_the_graffiti_on_ &lt;chr&gt;,\n#   where_is_the_graffiti_located_ &lt;chr&gt;, street_address &lt;chr&gt;, zip_code &lt;chr&gt;,\n#   x_coordinate &lt;chr&gt;, y_coordinate &lt;chr&gt;, ward &lt;chr&gt;, police_district &lt;chr&gt;,\n#   community_area &lt;chr&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, location &lt;df[,2]&gt;,\n#   `:@computed_region_awaf_s7ux` &lt;chr&gt;, `:@computed_region_6mkv_f3dw` &lt;chr&gt;,\n#   `:@computed_region_vrxf_vc4k` &lt;chr&gt;, `:@computed_region_bdys_3d7i` &lt;chr&gt;, …"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#converting-to-spatial-object",
    "href": "assignments/assignment_4/assignment4.html#converting-to-spatial-object",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Converting to Spatial Object",
    "text": "Converting to Spatial Object\n\n\nCode\n# Convert to sf object (WGS84)\ngraffiti_wgs84 &lt;- st_as_sf(graffiti_clean,\n                            coords = c(\"longitude\", \"latitude\"),\n                            crs = 4326)\n\n# Project to UTM Zone 16N for accurate distance calculations\ngraffiti_utm &lt;- st_transform(graffiti_wgs84, crs = 32616)\n\ncat(\"Spatial Object Created:\\n\")\n\n\nSpatial Object Created:\n\n\nCode\ncat(\"Total features:\", nrow(graffiti_utm), \"\\n\")\n\n\nTotal features: 49973"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#loading-chicago-city-boundary",
    "href": "assignments/assignment_4/assignment4.html#loading-chicago-city-boundary",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Loading Chicago City Boundary",
    "text": "Loading Chicago City Boundary\n\n\nCode\n# Load Chicago boundary from city's open data portal\nchicago_url &lt;- \"https://data.cityofchicago.org/api/geospatial/igwz-8jzy?method=export&format=GeoJSON\"\nchicago_boundary &lt;- st_read(chicago_url, quiet = TRUE)\n\n# Project to UTM Zone 16N\nchicago_utm &lt;- st_transform(chicago_boundary, crs = 32616)\n\ncat(\"Chicago Boundary Loaded\\n\")\n\n\nChicago Boundary Loaded\n\n\nCode\ncat(\"Area:\", round(as.numeric(st_area(chicago_utm))/1e6, 1), \"km²\\n\")\n\n\nArea: 4.8 9.1 6 6.6 5.3 8.1 8.2 7.1 2.9 11.3 6 8.3 6.5 5 10.2 8.3 9.6 2.6 10.1 3 5.1 9.3 9.3 11.8 18.5 3.4 5 14.7 8.3 11.9 7.6 4.3 4.6 2.6 4.3 1.6 1.8 4.5 2.7 3.9 4.2 5.4 7.6 7.6 3.2 8.7 1.6 4.5 12.5 5.5 28.2 7.7 9.2 9.1 13.5 10.9 5.2 7 3.7 5.4 12.5 3 5.7 6.6 7.6 9.1 8.2 8 9.2 12.6 9.8 8.2 7.4 7 8.5 34.5 4.5 km²"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#visualizing-spatial-distribution",
    "href": "assignments/assignment_4/assignment4.html#visualizing-spatial-distribution",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Visualizing Spatial Distribution",
    "text": "Visualizing Spatial Distribution\n\n\nCode\n# Create base map showing all graffiti points\nggplot() +\n  geom_sf(data = chicago_utm, fill = NA, color = \"black\", linewidth = 0.8) +\n  geom_sf(data = graffiti_utm, color = \"#E31C23\", size = 0.8, alpha = 0.3) +\n  theme_minimal() +\n  labs(\n    title = \"Spatial Distribution of Graffiti Removal Requests\",\n    subtitle = paste(\"Total requests:\", nrow(graffiti_utm)),\n    x = \"UTM Easting (m)\",\n    y = \"UTM Northing (m)\"\n  ) +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 11),\n    axis.text = element_text(size = 9)\n  )"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#temporal-patterns",
    "href": "assignments/assignment_4/assignment4.html#temporal-patterns",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Temporal Patterns",
    "text": "Temporal Patterns\n\n\nCode\n# Calculate monthly statistics\nmonthly_stats &lt;- graffiti_clean %&gt;%\n  group_by(month) %&gt;%\n  summarise(\n    count = n(),\n    avg_per_day = n() / n_distinct(date),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(month_name = month.abb[month])\n\n# Visualize monthly distribution\nggplot(monthly_stats, aes(x = reorder(month_name, month), y = count)) +\n  geom_col(fill = \"#E31C23\", alpha = 0.8, color = \"black\", linewidth = 0.3) +\n  geom_text(aes(label = count), vjust = -0.3, size = 3.5, fontface = \"bold\") +\n  theme_minimal() +\n  labs(\n    title = \"Monthly Distribution of Graffiti Requests\",\n    x = \"Month\",\n    y = \"Number of Requests\"\n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(size = 13, face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nCode\ncat(\"\\nTemporal Summary:\\n\")\n\n\n\nTemporal Summary:\n\n\nCode\nprint(monthly_stats)\n\n\n# A tibble: 7 × 4\n  month count avg_per_day month_name\n  &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt; &lt;chr&gt;     \n1     6  1337        334. Jun       \n2     7 10078        325. Jul       \n3     8 10098        326. Aug       \n4     9  8033        268. Sep       \n5    10  8808        284. Oct       \n6    11  7733        258. Nov       \n7    12  3886        216. Dec"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#creating-500m-500m-grid",
    "href": "assignments/assignment_4/assignment4.html#creating-500m-500m-grid",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Creating 500m × 500m Grid",
    "text": "Creating 500m × 500m Grid\n\n\nCode\n# Create regular grid covering Chicago\nfishnet_full &lt;- st_make_grid(\n  chicago_utm,\n  cellsize = 500,\n  square = TRUE,\n  what = \"polygons\"\n) %&gt;%\n  st_as_sf() %&gt;%\n  mutate(grid_id = row_number())\n\n# Keep only grid cells intersecting Chicago boundary\nfishnet_chicago &lt;- fishnet_full[chicago_utm, ]\n\ncat(\"Fishnet Grid Created:\\n\")\n\n\nFishnet Grid Created:\n\n\nCode\ncat(\"Total grid cells:\", nrow(fishnet_chicago), \"\\n\")\n\n\nTotal grid cells: 2618 \n\n\nCode\ncat(\"Cell size: 500m × 500m\\n\")\n\n\nCell size: 500m × 500m"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#aggregating-graffiti-counts-to-grid",
    "href": "assignments/assignment_4/assignment4.html#aggregating-graffiti-counts-to-grid",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Aggregating Graffiti Counts to Grid",
    "text": "Aggregating Graffiti Counts to Grid\n\n\nCode\n# Aggregate graffiti points to grid cells\ngraffiti_grid &lt;- fishnet_chicago %&gt;%\n  st_join(graffiti_utm, join = st_contains) %&gt;%\n  group_by(grid_id) %&gt;%\n  summarise(\n    n_graffiti = n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(n_graffiti = ifelse(is.na(n_graffiti), 0, n_graffiti))\n\ncat(\"Grid Aggregation Complete:\\n\")\n\n\nGrid Aggregation Complete:\n\n\nCode\ncat(\"Total cells:\", nrow(graffiti_grid), \"\\n\")\n\n\nTotal cells: 2618 \n\n\nCode\ncat(\"Cells with graffiti:\", sum(graffiti_grid$n_graffiti &gt; 0), \"\\n\")\n\n\nCells with graffiti: 2618 \n\n\nCode\ncat(\"Mean per cell:\", round(mean(graffiti_grid$n_graffiti), 2), \"\\n\\n\")\n\n\nMean per cell: 19.47 \n\n\nCode\nprint(summary(graffiti_grid$n_graffiti))\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    1.00    3.00   19.47   18.00  403.00"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#visualizing-grid-aggregation",
    "href": "assignments/assignment_4/assignment4.html#visualizing-grid-aggregation",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Visualizing Grid Aggregation",
    "text": "Visualizing Grid Aggregation\n\n\nCode\n# Map showing graffiti counts per grid cell\np1 &lt;- ggplot() +\n  geom_sf(data = graffiti_grid, aes(fill = n_graffiti), color = NA) +\n  geom_sf(data = chicago_utm, fill = NA, color = \"black\", linewidth = 0.5) +\n  scale_fill_viridis_c(name = \"Requests\", option = \"plasma\") +\n  theme_minimal() +\n  labs(\n    title = \"Graffiti Count Aggregation to 500m Grid\",\n    x = \"UTM Easting (m)\",\n    y = \"UTM Northing (m)\"\n  ) +\n  theme(plot.title = element_text(size = 13, face = \"bold\"))\n\n# Histogram of counts\np2 &lt;- ggplot(graffiti_grid, aes(x = n_graffiti)) +\n  geom_histogram(bins = 40, fill = \"#E31C23\", alpha = 0.8, color = \"black\") +\n  theme_minimal() +\n  labs(\n    title = \"Distribution of Graffiti Counts\",\n    x = \"Requests per Cell\",\n    y = \"Frequency\"\n  ) +\n  theme(plot.title = element_text(size = 12, face = \"bold\"))\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#creating-spatial-weights-matrix-and-features",
    "href": "assignments/assignment_4/assignment4.html#creating-spatial-weights-matrix-and-features",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Creating Spatial Weights Matrix and Features",
    "text": "Creating Spatial Weights Matrix and Features\n\n\nCode\n# Step 1: Ensure graffiti_grid is sf with grid_id\ngraffiti_grid &lt;- graffiti_grid %&gt;%\n  mutate(grid_id = row_number())\n\n# Step 2: Convert to sp object (required by spdep)\ngraffiti_sp &lt;- as_Spatial(graffiti_grid)\n\n# Step 3: Create k-nearest neighbors weight matrix\nknn_neighbors &lt;- knearneigh(coordinates(graffiti_sp), k = 5)\nknn_weights &lt;- knn2nb(knn_neighbors, sym = TRUE)\nknn_weights_std &lt;- nb2listw(knn_weights, style = \"W\")\n\ncat(\"Spatial Weights Matrix Created (k=5 nearest neighbors)\\n\")\n\n\nSpatial Weights Matrix Created (k=5 nearest neighbors)\n\n\nCode\n# Step 4: Calculate k-NN feature (neighbor mean)\nneighbor_lags &lt;- lag.listw(knn_weights_std, graffiti_grid$n_graffiti)\n\n# Step 5: Calculate Local Moran's I\nlocal_moran &lt;- localmoran(graffiti_grid$n_graffiti, knn_weights_std)\n\n# Step 6: Calculate distance to hotspot\nhotspot_threshold &lt;- quantile(graffiti_grid$n_graffiti, 0.75)\nhotspots &lt;- graffiti_grid %&gt;% filter(n_graffiti &gt;= hotspot_threshold)\nhotspot_center &lt;- st_centroid(st_union(hotspots))\ndist_to_hotspot &lt;- as.numeric(st_distance(graffiti_grid, hotspot_center)) / 1000\n\n# Step 7: Add all features to graffiti_grid\ngraffiti_grid &lt;- graffiti_grid %&gt;%\n  mutate(\n    neighbor_mean = neighbor_lags,\n    local_i = local_moran[, 1],\n    local_i_pval = local_moran[, 5],\n    dist_to_hotspot = dist_to_hotspot,\n    moran_cluster = case_when(\n      local_i_pval &gt;= 0.05 ~ \"Not significant\",\n      local_i &gt;= 0 & n_graffiti &gt;= median(n_graffiti) ~ \"High-High\",\n      local_i &gt;= 0 & n_graffiti &lt; median(n_graffiti) ~ \"Low-Low\",\n      local_i &lt; 0 & n_graffiti &gt;= median(n_graffiti) ~ \"High-Low\",\n      TRUE ~ \"Low-High\"\n    )\n  )\n\ncat(\"\\nSpatial Features Added:\\n\")\n\n\n\nSpatial Features Added:\n\n\nCode\ncat(\"neighbor_mean: Average graffiti count in 5 nearest neighbors\\n\")\n\n\nneighbor_mean: Average graffiti count in 5 nearest neighbors\n\n\nCode\ncat(\"local_i: Local Moran's I statistic\\n\")\n\n\nlocal_i: Local Moran's I statistic\n\n\nCode\ncat(\"dist_to_hotspot: Distance to graffiti hotspot (km)\\n\")\n\n\ndist_to_hotspot: Distance to graffiti hotspot (km)\n\n\nCode\ncat(\"moran_cluster: Classification of spatial clusters\\n\")\n\n\nmoran_cluster: Classification of spatial clusters\n\n\nCode\ncat(\"\\nFirst 5 rows:\\n\")\n\n\n\nFirst 5 rows:\n\n\nCode\nfeature_summary &lt;- graffiti_grid %&gt;% \n  st_drop_geometry()\n\nhead(feature_summary, 5)\n\n\n# A tibble: 5 × 7\n  grid_id n_graffiti neighbor_mean local_i local_i_pval dist_to_hotspot\n    &lt;int&gt;      &lt;int&gt;         &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;\n1       1          1             1   0.200        0.317            27.9\n2       2          1             1   0.200        0.317            28.0\n3       3          1             1   0.200        0.273            28.1\n4       4          1             1   0.200        0.317            28.2\n5       5          1             1   0.200        0.317            28.3\n# ℹ 1 more variable: moran_cluster &lt;chr&gt;"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#visualizing-hotspots-and-coldspots",
    "href": "assignments/assignment_4/assignment4.html#visualizing-hotspots-and-coldspots",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Visualizing Hotspots and Coldspots",
    "text": "Visualizing Hotspots and Coldspots\n\n\nCode\nggplot() +\n  geom_sf(data = graffiti_grid, aes(fill = moran_cluster), color = NA) +\n  geom_sf(data = chicago_utm, fill = NA, color = \"black\", linewidth = 0.5) +\n  scale_fill_manual(\n    name = \"Cluster Type\",\n    values = c(\n      \"High-High\" = \"#d73027\",\n      \"Low-Low\" = \"#91bfdb\",\n      \"High-Low\" = \"#fee090\",\n      \"Low-High\" = \"#a6d96a\",\n      \"Not significant\" = \"#f7f7f7\"\n    )\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Local Moran's I: Graffiti Hotspots and Coldspots\",\n    x = \"UTM Easting (m)\",\n    y = \"UTM Northing (m)\"\n  ) +\n  theme(plot.title = element_text(size = 13, face = \"bold\"))\n\n\n\n\n\n\n\n\n\nCode\ncat(\"\\nCluster Distribution:\\n\")\n\n\n\nCluster Distribution:\n\n\nCode\nprint(table(graffiti_grid$moran_cluster))\n\n\n\n      High-High        High-Low        Low-High Not significant \n            304              20               2            2292"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#preparing-data-for-modeling",
    "href": "assignments/assignment_4/assignment4.html#preparing-data-for-modeling",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Preparing Data for Modeling",
    "text": "Preparing Data for Modeling\n\n\nCode\n# Prepare data for modeling\nmodel_data &lt;- graffiti_grid %&gt;%\n  st_drop_geometry() %&gt;%\n  na.omit()\n\ncat(\"Model Data Prepared:\\n\")\n\n\nModel Data Prepared:\n\n\nCode\ncat(\"Sample size:\", nrow(model_data), \"\\n\")\n\n\nSample size: 2618 \n\n\nCode\ncat(\"Dependent variable (n_graffiti):\\n\")\n\n\nDependent variable (n_graffiti):\n\n\nCode\nprint(summary(model_data$n_graffiti))\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    1.00    3.00   19.47   18.00  403.00 \n\n\nCode\ncat(\"\\nOverdispersion check:\\n\")\n\n\n\nOverdispersion check:\n\n\nCode\ncat(\"Mean:\", mean(model_data$n_graffiti), \"\\n\")\n\n\nMean: 19.46639 \n\n\nCode\ncat(\"Variance:\", var(model_data$n_graffiti), \"\\n\")\n\n\nVariance: 1706.421 \n\n\nCode\ncat(\"Variance/Mean ratio:\", round(var(model_data$n_graffiti)/mean(model_data$n_graffiti), 2), \"\\n\")\n\n\nVariance/Mean ratio: 87.66"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#poisson-regression",
    "href": "assignments/assignment_4/assignment4.html#poisson-regression",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Poisson Regression",
    "text": "Poisson Regression\n\n\nCode\n# Fit Poisson regression\npoisson_model &lt;- glm(\n  n_graffiti ~ neighbor_mean + local_i + dist_to_hotspot,\n  family = poisson(link = \"log\"),\n  data = model_data\n)\n\ncat(\"POISSON REGRESSION RESULTS\\n\")\n\n\nPOISSON REGRESSION RESULTS\n\n\nCode\ncat(strrep(\"=\", 50), \"\\n\\n\")\n\n\n================================================== \n\n\nCode\nprint(summary(poisson_model))\n\n\n\nCall:\nglm(formula = n_graffiti ~ neighbor_mean + local_i + dist_to_hotspot, \n    family = poisson(link = \"log\"), data = model_data)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      3.3068979  0.0126373  261.68   &lt;2e-16 ***\nneighbor_mean    0.0121075  0.0001187  101.97   &lt;2e-16 ***\nlocal_i          0.0780056  0.0009253   84.30   &lt;2e-16 ***\ndist_to_hotspot -0.0971570  0.0010339  -93.97   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 119805  on 2617  degrees of freedom\nResidual deviance:  36856  on 2614  degrees of freedom\nAIC: 45856\n\nNumber of Fisher Scoring iterations: 5\n\n\nCode\n# Extract metrics\npoisson_aic &lt;- AIC(poisson_model)\npoisson_deviance &lt;- deviance(poisson_model)\npoisson_dispersion &lt;- poisson_deviance / df.residual(poisson_model)\n\ncat(\"\\nModel Fit Statistics:\\n\")\n\n\n\nModel Fit Statistics:\n\n\nCode\ncat(\"AIC:\", round(poisson_aic, 2), \"\\n\")\n\n\nAIC: 45856.21 \n\n\nCode\ncat(\"Dispersion statistic:\", round(poisson_dispersion, 3), \"\\n\")\n\n\nDispersion statistic: 14.099"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#negative-binomial-regression",
    "href": "assignments/assignment_4/assignment4.html#negative-binomial-regression",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Negative Binomial Regression",
    "text": "Negative Binomial Regression\n\n\nCode\n# Fit Negative Binomial regression\nnb_model &lt;- glm.nb(\n  n_graffiti ~ neighbor_mean + local_i + dist_to_hotspot,\n  data = model_data\n)\n\ncat(\"\\nNEGATIVE BINOMIAL REGRESSION RESULTS\\n\")\n\n\n\nNEGATIVE BINOMIAL REGRESSION RESULTS\n\n\nCode\ncat(strrep(\"=\", 50), \"\\n\\n\")\n\n\n================================================== \n\n\nCode\nprint(summary(nb_model))\n\n\n\nCall:\nglm.nb(formula = n_graffiti ~ neighbor_mean + local_i + dist_to_hotspot, \n    data = model_data, init.theta = 1.254353791, link = log)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      2.4862158  0.0557367  44.606   &lt;2e-16 ***\nneighbor_mean    0.0326243  0.0008968  36.378   &lt;2e-16 ***\nlocal_i         -0.0161810  0.0121606  -1.331    0.183    \ndist_to_hotspot -0.0826818  0.0034258 -24.135   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.2544) family taken to be 1)\n\n    Null deviance: 8149.6  on 2617  degrees of freedom\nResidual deviance: 2538.9  on 2614  degrees of freedom\nAIC: 16295\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.2544 \n          Std. Err.:  0.0385 \n\n 2 x log-likelihood:  -16285.0510 \n\n\nCode\n# Extract metrics\nnb_aic &lt;- AIC(nb_model)\nnb_deviance &lt;- deviance(nb_model)\n\ncat(\"\\nModel Fit Statistics:\\n\")\n\n\n\nModel Fit Statistics:\n\n\nCode\ncat(\"AIC:\", round(nb_aic, 2), \"\\n\")\n\n\nAIC: 16295.05"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#model-comparison",
    "href": "assignments/assignment_4/assignment4.html#model-comparison",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Model Comparison",
    "text": "Model Comparison\n\n\nCode\n# Compare models\ncat(\"\\nMODEL COMPARISON\\n\")\n\n\n\nMODEL COMPARISON\n\n\nCode\ncat(strrep(\"=\", 50), \"\\n\")\n\n\n================================================== \n\n\nCode\ncat(\"Poisson AIC:\", round(poisson_aic, 2), \"\\n\")\n\n\nPoisson AIC: 45856.21 \n\n\nCode\ncat(\"Negative Binomial AIC:\", round(nb_aic, 2), \"\\n\")\n\n\nNegative Binomial AIC: 16295.05 \n\n\nCode\ncat(\"Difference:\", round(poisson_aic - nb_aic, 2), \"\\n\\n\")\n\n\nDifference: 29561.15 \n\n\nCode\nif (nb_aic &lt; poisson_aic) {\n  cat(\"Decision: Negative Binomial model preferred (lower AIC)\\n\")\n  selected_model &lt;- nb_model\n  model_name &lt;- \"Negative Binomial\"\n} else {\n  cat(\"Decision: Poisson model preferred (lower AIC)\\n\")\n  selected_model &lt;- poisson_model\n  model_name &lt;- \"Poisson\"\n}\n\n\nDecision: Negative Binomial model preferred (lower AIC)\n\n\nCode\n# Extract coefficients\ncoef_table &lt;- tidy(selected_model, exponentiate = TRUE, conf.int = TRUE)\n\ncat(\"\\n\\nSelected Model Coefficients (Exponentiated - Incidence Rate Ratios):\\n\")\n\n\n\n\nSelected Model Coefficients (Exponentiated - Incidence Rate Ratios):\n\n\nCode\nprint(coef_table)\n\n\n# A tibble: 4 × 7\n  term            estimate std.error statistic   p.value conf.low conf.high\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)       12.0    0.0557       44.6  0           10.6      13.6  \n2 neighbor_mean      1.03   0.000897     36.4  9.44e-290    1.03      1.04 \n3 local_i            0.984  0.0122       -1.33 1.83e-  1    0.960     1.01 \n4 dist_to_hotspot    0.921  0.00343     -24.1  1.07e-128    0.914     0.927"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#setting-up-spatial-folds",
    "href": "assignments/assignment_4/assignment4.html#setting-up-spatial-folds",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Setting Up Spatial Folds",
    "text": "Setting Up Spatial Folds\n\n\nCode\n# Create spatial folds\nset.seed(123)\nn_folds &lt;- 5\n\ncentroids &lt;- st_coordinates(st_centroid(graffiti_grid))\nspatial_folds &lt;- kmeans(centroids, centers = n_folds)$cluster\n\ncv_data &lt;- model_data %&gt;%\n  mutate(fold = spatial_folds[row_number()])\n\ncat(\"Spatial Cross-Validation Setup:\\n\")\n\n\nSpatial Cross-Validation Setup:\n\n\nCode\ncat(\"Number of folds:\", n_folds, \"\\n\")\n\n\nNumber of folds: 5 \n\n\nCode\ncat(\"Fold sizes:\\n\")\n\n\nFold sizes:\n\n\nCode\nprint(table(cv_data$fold))\n\n\n\n  1   2   3   4   5 \n497 331 505 610 675"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#running-cross-validation",
    "href": "assignments/assignment_4/assignment4.html#running-cross-validation",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Running Cross-Validation",
    "text": "Running Cross-Validation\n\n\nCode\n# Cross-validation loop\ncv_results &lt;- tibble()\n\nfor (fold_num in 1:n_folds) {\n  train_data &lt;- cv_data %&gt;% filter(fold != fold_num)\n  test_data &lt;- cv_data %&gt;% filter(fold == fold_num)\n  \n  # Fit model on training data\n  if (model_name == \"Negative Binomial\") {\n    fold_model &lt;- glm.nb(\n      n_graffiti ~ neighbor_mean + local_i + dist_to_hotspot,\n      data = train_data\n    )\n  } else {\n    fold_model &lt;- glm(\n      n_graffiti ~ neighbor_mean + local_i + dist_to_hotspot,\n      family = poisson(link = \"log\"),\n      data = train_data\n    )\n  }\n  \n  # Predictions\n  pred &lt;- predict(fold_model, newdata = test_data, type = \"response\")\n  \n  fold_results &lt;- test_data %&gt;%\n    mutate(\n      fold = fold_num,\n      observed = n_graffiti,\n      predicted = pred,\n      error = observed - predicted,\n      abs_error = abs(error),\n      squared_error = error^2\n    )\n  \n  cv_results &lt;- bind_rows(cv_results, fold_results)\n}\n\ncat(\"Cross-Validation Complete:\\n\")\n\n\nCross-Validation Complete:\n\n\nCode\ncat(\"Total predictions:\", nrow(cv_results), \"\\n\")\n\n\nTotal predictions: 2618"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#cross-validation-error-metrics",
    "href": "assignments/assignment_4/assignment4.html#cross-validation-error-metrics",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Cross-Validation Error Metrics",
    "text": "Cross-Validation Error Metrics\n\n\nCode\n# Calculate error metrics\nmae &lt;- mean(cv_results$abs_error, na.rm = TRUE)\nrmse &lt;- sqrt(mean(cv_results$squared_error, na.rm = TRUE))\nme &lt;- mean(cv_results$error, na.rm = TRUE)\n\ncat(\"SPATIAL CROSS-VALIDATION RESULTS\\n\")\n\n\nSPATIAL CROSS-VALIDATION RESULTS\n\n\nCode\ncat(strrep(\"=\", 50), \"\\n\")\n\n\n================================================== \n\n\nCode\ncat(\"Mean Absolute Error (MAE):\", round(mae, 3), \"\\n\")\n\n\nMean Absolute Error (MAE): 44.017 \n\n\nCode\ncat(\"Root Mean Squared Error (RMSE):\", round(rmse, 3), \"\\n\")\n\n\nRoot Mean Squared Error (RMSE): 383.165 \n\n\nCode\ncat(\"Mean Error (ME):\", round(me, 3), \"\\n\")\n\n\nMean Error (ME): -33.302"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#visualizing-cv-results",
    "href": "assignments/assignment_4/assignment4.html#visualizing-cv-results",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Visualizing CV Results",
    "text": "Visualizing CV Results\n\n\nCode\n# Observed vs Predicted\np1 &lt;- ggplot(cv_results, aes(x = observed, y = predicted)) +\n  geom_point(alpha = 0.5, color = \"#E31C23\") +\n  geom_abline(intercept = 0, slope = 1, color = \"blue\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(\n    title = \"Spatial CV: Observed vs Predicted\",\n    x = \"Observed\",\n    y = \"Predicted\",\n    subtitle = paste(\"MAE =\", round(mae, 2))\n  ) +\n  theme(plot.title = element_text(size = 12, face = \"bold\"))\n\n# Residuals\np2 &lt;- ggplot(cv_results, aes(x = error)) +\n  geom_histogram(bins = 30, fill = \"#E31C23\", alpha = 0.7) +\n  geom_vline(xintercept = 0, color = \"blue\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(\n    title = \"Residuals Distribution\",\n    x = \"Prediction Error\",\n    y = \"Frequency\"\n  ) +\n  theme(plot.title = element_text(size = 12, face = \"bold\"))\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#loading-2018-crime-data",
    "href": "assignments/assignment_4/assignment4.html#loading-2018-crime-data",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Loading 2018 Crime Data",
    "text": "Loading 2018 Crime Data\n\n\nCode\n# Download 2018 crimes data\ncrime_url &lt;- \"https://data.cityofchicago.org/resource/3i3m-jwuy.json\"\n\n# Simple query without complex filters\nresponse_2018 &lt;- GET(crime_url, query = list(\"$limit\" = 50000))\ncrime_2018_raw &lt;- fromJSON(content(response_2018, \"text\"))\ncrime_2018_df &lt;- as_tibble(crime_2018_raw)\n\ncat(\"2018 Crime Data Loaded:\\n\")\n\n\n2018 Crime Data Loaded:\n\n\nCode\ncat(\"Total records:\", nrow(crime_2018_df), \"\\n\")\n\n\nTotal records: 50000 \n\n\nCode\ncat(\"Columns:\", ncol(crime_2018_df), \"\\n\")\n\n\nColumns: 22"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#preparing-2018-data",
    "href": "assignments/assignment_4/assignment4.html#preparing-2018-data",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Preparing 2018 Data",
    "text": "Preparing 2018 Data\n\n\nCode\n# Clean 2018 data\ncrime_2018_clean &lt;- crime_2018_df %&gt;%\n  filter(!is.na(longitude) & !is.na(latitude)) %&gt;%\n  mutate(\n    longitude = as.numeric(longitude),\n    latitude = as.numeric(latitude)\n  ) %&gt;%\n  filter(longitude &gt; -88.5 & longitude &lt; -87.5,\n         latitude &gt; 41.6 & latitude &lt; 42.1)\n\n# Convert to sf\ncrime_2018_wgs84 &lt;- st_as_sf(crime_2018_clean,\n                             coords = c(\"longitude\", \"latitude\"),\n                             crs = 4326)\n\ncrime_2018_utm &lt;- st_transform(crime_2018_wgs84, crs = 32616)\n\ncat(\"2018 Data Cleaned:\\n\")\n\n\n2018 Data Cleaned:\n\n\nCode\ncat(\"Records:\", nrow(crime_2018_utm), \"\\n\")\n\n\nRecords: 48166"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#aggregating-2018-to-same-grid",
    "href": "assignments/assignment_4/assignment4.html#aggregating-2018-to-same-grid",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Aggregating 2018 to Same Grid",
    "text": "Aggregating 2018 to Same Grid\n\n\nCode\n# Aggregate to grid\ncrime_2018_grid &lt;- fishnet_chicago %&gt;%\n  st_join(crime_2018_utm, join = st_contains) %&gt;%\n  group_by(grid_id) %&gt;%\n  summarise(\n    n_crime = n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(n_crime = ifelse(is.na(n_crime), 0, n_crime))\n\ncat(\"2018 Aggregation:\\n\")\n\n\n2018 Aggregation:\n\n\nCode\ncat(\"Total cells:\", nrow(crime_2018_grid), \"\\n\")\n\n\nTotal cells: 2618 \n\n\nCode\ncat(\"Cells with crimes:\", sum(crime_2018_grid$n_crime &gt; 0), \"\\n\")\n\n\nCells with crimes: 2618"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#predicting-2018-with-2017-model",
    "href": "assignments/assignment_4/assignment4.html#predicting-2018-with-2017-model",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Predicting 2018 with 2017 Model",
    "text": "Predicting 2018 with 2017 Model\n\n\nCode\n# Prepare features\ncrime_2018_features &lt;- crime_2018_grid %&gt;%\n  st_drop_geometry() %&gt;%\n  mutate(\n    observed_2018 = n_crime\n  ) %&gt;%\n  left_join(\n    model_data %&gt;% mutate(grid_id = row_number()),\n    by = \"grid_id\"\n  ) %&gt;%\n  na.omit()\n\ncat(\"2018 Features Prepared:\\n\")\n\n\n2018 Features Prepared:\n\n\nCode\ncat(\"Records:\", nrow(crime_2018_features), \"\\n\")\n\n\nRecords: 1186 \n\n\nCode\n# Make predictions using 2017 model\npred_2018 &lt;- predict(selected_model, \n                     newdata = crime_2018_features, \n                     type = \"response\")\n\ntemporal_results &lt;- crime_2018_features %&gt;%\n  mutate(\n    predicted_2018 = pred_2018,\n    error_2018 = observed_2018 - predicted_2018,\n    abs_error_2018 = abs(error_2018),\n    squared_error_2018 = error_2018^2\n  )\n\ncat(\"Temporal Validation:\\n\")\n\n\nTemporal Validation:\n\n\nCode\ncat(\"Predictions made:\", nrow(temporal_results), \"\\n\")\n\n\nPredictions made: 1186"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#temporal-validation-metrics",
    "href": "assignments/assignment_4/assignment4.html#temporal-validation-metrics",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Temporal Validation Metrics",
    "text": "Temporal Validation Metrics\n\n\nCode\n# Calculate 2018 metrics\nmae_2018 &lt;- mean(temporal_results$abs_error_2018)\nrmse_2018 &lt;- sqrt(mean(temporal_results$squared_error_2018))\n\ncat(\"TEMPORAL VALIDATION (2018)\\n\")\n\n\nTEMPORAL VALIDATION (2018)\n\n\nCode\ncat(strrep(\"=\", 50), \"\\n\")\n\n\n================================================== \n\n\nCode\ncat(\"MAE:\", round(mae_2018, 3), \"\\n\")\n\n\nMAE: 41.618 \n\n\nCode\ncat(\"RMSE:\", round(rmse_2018, 3), \"\\n\")\n\n\nRMSE: 201.111"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#calculating-kde-baseline",
    "href": "assignments/assignment_4/assignment4.html#calculating-kde-baseline",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Calculating KDE Baseline",
    "text": "Calculating KDE Baseline\n\n\nCode\n# Create point pattern\ngraffiti_coords &lt;- st_coordinates(graffiti_utm)\n\n# Create window\nwindow &lt;- owin(xrange = range(graffiti_coords[, 1]),\n               yrange = range(graffiti_coords[, 2]))\n\n# Create point pattern\ngraffiti_ppp &lt;- ppp(x = graffiti_coords[, 1],\n                    y = graffiti_coords[, 2],\n                    window = window)\n\n# Calculate KDE\nkde_2017 &lt;- density(graffiti_ppp, sigma = bw.diggle(graffiti_ppp))\nkde_raster &lt;- raster(kde_2017)\n\n# Extract to grid\ngrid_centroids &lt;- st_centroid(graffiti_grid)\nkde_values &lt;- raster::extract(kde_raster, as_Spatial(grid_centroids))\n\n# Normalize\nkde_grid_counts &lt;- kde_values * (500 * 500) / sum(kde_values, na.rm = TRUE) * \n                   sum(graffiti_grid$n_graffiti)\n\n# Calculate errors\nkde_errors &lt;- graffiti_grid %&gt;%\n  st_drop_geometry() %&gt;%\n  mutate(\n    predicted_kde = kde_grid_counts,\n    error_kde = n_graffiti - predicted_kde,\n    abs_error_kde = abs(error_kde),\n    squared_error_kde = error_kde^2\n  ) %&gt;%\n  na.omit()\n\nmae_kde &lt;- mean(kde_errors$abs_error_kde)\nrmse_kde &lt;- sqrt(mean(kde_errors$squared_error_kde))\n\ncat(\"KDE BASELINE\\n\")\n\n\nKDE BASELINE\n\n\nCode\ncat(strrep(\"=\", 50), \"\\n\")\n\n\n================================================== \n\n\nCode\ncat(\"MAE:\", round(mae_kde, 3), \"\\n\")\n\n\nMAE: 5204537 \n\n\nCode\ncat(\"RMSE:\", round(rmse_kde, 3), \"\\n\")\n\n\nRMSE: 13632995"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#comprehensive-comparison",
    "href": "assignments/assignment_4/assignment4.html#comprehensive-comparison",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Comprehensive Comparison",
    "text": "Comprehensive Comparison\n\n\nCode\n# Compare all methods\ncomparison &lt;- tibble(\n  Method = c(\n    paste(\"Spatial CV -\", model_name),\n    \"Temporal (2018)\",\n    \"KDE Baseline\"\n  ),\n  MAE = c(mae, mae_2018, mae_kde),\n  RMSE = c(rmse, rmse_2018, rmse_kde)\n)\n\ncat(\"FINAL MODEL COMPARISON\\n\")\n\n\nFINAL MODEL COMPARISON\n\n\nCode\ncat(strrep(\"=\", 50), \"\\n\")\n\n\n================================================== \n\n\nCode\nprint(comparison)\n\n\n# A tibble: 3 × 3\n  Method                               MAE      RMSE\n  &lt;chr&gt;                              &lt;dbl&gt;     &lt;dbl&gt;\n1 Spatial CV - Negative Binomial      44.0      383.\n2 Temporal (2018)                     41.6      201.\n3 KDE Baseline                   5204537.  13632995.\n\n\nCode\ncat(\"\\nImprovement Over KDE Baseline:\\n\")\n\n\n\nImprovement Over KDE Baseline:\n\n\nCode\ncat(\"Spatial CV:\", round((1 - mae/mae_kde)*100, 1), \"%\\n\")\n\n\nSpatial CV: 100 %\n\n\nCode\ncat(\"Temporal:\", round((1 - mae_2018/mae_kde)*100, 1), \"%\\n\")\n\n\nTemporal: 100 %\n\n\nCode\n# Visualize comparison\ncomparison_long &lt;- comparison %&gt;%\n  pivot_longer(cols = c(MAE, RMSE), names_to = \"Metric\", values_to = \"Value\")\n\nggplot(comparison_long, aes(x = Method, y = Value, fill = Metric)) +\n  geom_col(position = \"dodge\", alpha = 0.8, color = \"black\") +\n  geom_text(aes(label = round(Value, 2)), position = position_dodge(width = 0.9), \n            vjust = -0.3, size = 3.5) +\n  theme_minimal() +\n  labs(\n    title = \"Final Model Comparison: Error Metrics\",\n    y = \"Error (lower is better)\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme(\n    plot.title = element_text(size = 13, face = \"bold\"),\n    axis.text.x = element_text(angle = 30, hjust = 1)\n  )"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#mapping-prediction-errors",
    "href": "assignments/assignment_4/assignment4.html#mapping-prediction-errors",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Mapping Prediction Errors",
    "text": "Mapping Prediction Errors\n\n\nCode\n# Add predictions back to grid\ngraffiti_grid_errors &lt;- graffiti_grid %&gt;%\n  left_join(\n    cv_results %&gt;%\n      group_by(grid_id) %&gt;%\n      summarise(\n        predicted = mean(predicted, na.rm = TRUE),\n        error = mean(error, na.rm = TRUE),\n        abs_error = mean(abs_error, na.rm = TRUE),\n        .groups = \"drop\"\n      ),\n    by = \"grid_id\"\n  )\n\n# Map absolute errors\np1 &lt;- ggplot() +\n  geom_sf(data = graffiti_grid_errors, aes(fill = abs_error), color = NA) +\n  geom_sf(data = chicago_utm, fill = NA, color = \"black\", linewidth = 0.5) +\n  scale_fill_viridis_c(name = \"Absolute Error\", option = \"magma\", na.value = \"white\") +\n  theme_minimal() +\n  labs(\n    title = \"Geographic Distribution of Absolute Prediction Errors\",\n    x = \"UTM Easting (m)\",\n    y = \"UTM Northing (m)\"\n  ) +\n  theme(plot.title = element_text(size = 12, face = \"bold\"))\n\n# Map signed errors\np2 &lt;- ggplot() +\n  geom_sf(data = graffiti_grid_errors, aes(fill = error), color = NA) +\n  geom_sf(data = chicago_utm, fill = NA, color = \"black\", linewidth = 0.5) +\n  scale_fill_distiller(\n    name = \"Error\",\n    type = \"div\",\n    palette = \"RdBu\",\n    na.value = \"white\"\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Signed Errors: Red=Underpredicted, Blue=Overpredicted\",\n    x = \"UTM Easting (m)\",\n    y = \"UTM Northing (m)\"\n  ) +\n  theme(plot.title = element_text(size = 12, face = \"bold\"))\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#performance-by-observed-count-level",
    "href": "assignments/assignment_4/assignment4.html#performance-by-observed-count-level",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Performance by Observed Count Level",
    "text": "Performance by Observed Count Level\n\n\nCode\n# Analyze error patterns\nerror_by_count &lt;- cv_results %&gt;%\n  mutate(count_bin = cut(observed, \n                        breaks = c(-Inf, 0, 5, 10, 20, Inf),\n                        labels = c(\"0\", \"1-5\", \"6-10\", \"11-20\", \"20+\"))) %&gt;%\n  group_by(count_bin) %&gt;%\n  summarise(\n    n = n(),\n    MAE = mean(abs_error, na.rm = TRUE),\n    RMSE = sqrt(mean(squared_error, na.rm = TRUE)),\n    .groups = \"drop\"\n  )\n\ncat(\"Error Metrics by Observed Count Level:\\n\\n\")\n\n\nError Metrics by Observed Count Level:\n\n\nCode\nprint(error_by_count)\n\n\n# A tibble: 4 × 4\n  count_bin     n    MAE  RMSE\n  &lt;fct&gt;     &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 1-5        1543   3.33  12.2\n2 6-10        247  22.5  188. \n3 11-20       205  71.9  871. \n4 20+         623 144.   594. \n\n\nCode\nggplot(error_by_count, aes(x = count_bin)) +\n  geom_col(aes(y = MAE), fill = \"#E31C23\", alpha = 0.7) +\n  geom_point(aes(y = RMSE), color = \"#0073C2\", size = 4) +\n  theme_minimal() +\n  labs(\n    title = \"Prediction Error by Observed Graffiti Count Level\",\n    x = \"Observed Count Range\",\n    y = \"Error Metric\",\n    subtitle = \"Bars = MAE | Points = RMSE\"\n  ) +\n  theme(plot.title = element_text(size = 12, face = \"bold\"))"
  },
  {
    "objectID": "assignments/assignment_4/assignment4.html#key-findings",
    "href": "assignments/assignment_4/assignment4.html#key-findings",
    "title": "Spatial Predictive Analysis: Chicago Graffiti Removal 311 Requests",
    "section": "Key Findings",
    "text": "Key Findings\n\n\nCode\ncat(\"ANALYSIS SUMMARY\\n\")\n\n\nANALYSIS SUMMARY\n\n\nCode\ncat(strrep(\"=\", 50), \"\\n\\n\")\n\n\n================================================== \n\n\nCode\ncat(\"1. DATA OVERVIEW\\n\")\n\n\n1. DATA OVERVIEW\n\n\nCode\ncat(\"   • Graffiti requests analyzed:\", nrow(graffiti_utm), \"\\n\")\n\n\n   • Graffiti requests analyzed: 49973 \n\n\nCode\ncat(\"   • Grid cells created:\", nrow(graffiti_grid), \"\\n\")\n\n\n   • Grid cells created: 2618 \n\n\nCode\ncat(\"   • Cells with graffiti:\", sum(graffiti_grid$n_graffiti &gt; 0), \"\\n\\n\")\n\n\n   • Cells with graffiti: 2618 \n\n\nCode\ncat(\"2. SPATIAL PATTERNS\\n\")\n\n\n2. SPATIAL PATTERNS\n\n\nCode\ncat(\"   • Significant clusters identified:\", \n    sum(graffiti_grid$local_i_pval &lt; 0.05), \"\\n\")\n\n\n   • Significant clusters identified: 326 \n\n\nCode\ncat(\"   • Hotspot areas (High-High):\", \n    sum(graffiti_grid$moran_cluster == \"High-High\"), \"\\n\")\n\n\n   • Hotspot areas (High-High): 304 \n\n\nCode\ncat(\"   • Coldspot areas (Low-Low):\", \n    sum(graffiti_grid$moran_cluster == \"Low-Low\"), \"\\n\\n\")\n\n\n   • Coldspot areas (Low-Low): 0 \n\n\nCode\ncat(\"3. MODEL PERFORMANCE\\n\")\n\n\n3. MODEL PERFORMANCE\n\n\nCode\ncat(\"   • Selected model:\", model_name, \"\\n\")\n\n\n   • Selected model: Negative Binomial \n\n\nCode\ncat(\"   • Spatial CV MAE:\", round(mae, 3), \"\\n\")\n\n\n   • Spatial CV MAE: 44.017 \n\n\nCode\ncat(\"   • Spatial CV RMSE:\", round(rmse, 3), \"\\n\")\n\n\n   • Spatial CV RMSE: 383.165 \n\n\nCode\ncat(\"   • Improvement over KDE:\", round((1 - mae/mae_kde)*100, 1), \"%\\n\\n\")\n\n\n   • Improvement over KDE: 100 %\n\n\nCode\ncat(\"4. TEMPORAL STABILITY\\n\")\n\n\n4. TEMPORAL STABILITY\n\n\nCode\ncat(\"   • 2018 predictions MAE:\", round(mae_2018, 3), \"\\n\")\n\n\n   • 2018 predictions MAE: 41.618 \n\n\nCode\nif (mae_2018 &lt; mae * 1.3) {\n  cat(\"   • Model shows good temporal stability\\n\")\n} else {\n  cat(\"   • Model shows temporal degradation\\n\")\n}\n\n\n   • Model shows good temporal stability\n\n\nCode\ncat(\"\\n5. SPATIAL FEATURES IMPORTANCE\\n\")\n\n\n\n5. SPATIAL FEATURES IMPORTANCE\n\n\nCode\nfor (i in 2:nrow(coef_table)) {\n  term &lt;- coef_table$term[i]\n  irr &lt;- coef_table$estimate[i]\n  pval &lt;- coef_table$p.value[i]\n  sig &lt;- ifelse(pval &lt; 0.05, \"***\", \"\")\n  cat(\"   •\", term, \"(IRR =\", round(irr, 3), \") \", sig, \"\\n\")\n}\n\n\n   • neighbor_mean (IRR = 1.033 )  *** \n   • local_i (IRR = 0.984 )   \n   • dist_to_hotspot (IRR = 0.921 )  *** \n\n\n\nReport Generated: 2025-11-16 01:44:22\nCourse: MUSA 5080 - Public Policy Analytics\nAssignment: Lab 4 - Spatial Predictive Analysis\nInstitution: University of Pennsylvania"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html",
    "href": "assignments/assignment_1/assignment1.html",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Michigan Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#scenario",
    "href": "assignments/assignment_1/assignment1.html#scenario",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the Michigan Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#learning-objectives",
    "href": "assignments/assignment_1/assignment1.html#learning-objectives",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#submission-instructions",
    "href": "assignments/assignment_1/assignment1.html#submission-instructions",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#data-retrieval",
    "href": "assignments/assignment_1/assignment1.html#data-retrieval",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\ncounty_data_2022 &lt;- get_acs(geography = \"county\", \n                         state = \"MI\",\n                         variables = c(med_household_income = \"B19013_001\", total_pop = \"B01003_001\" ),\n                         year = 2022,\n                         survey = \"acs5\",\n                         output = \"wide\")\n                         \n\n# Clean the county names to remove state name and \"County\" \ncounty_data_2022 &lt;- county_data_2022 %&gt;%\n  mutate(county_data_2022_clean = str_remove(NAME,paste0(\"County, Michigan\")))\n# Hint: use mutate() with str_remove()\n\n# Display the first few rows\nhead(county_data_2022)\n\n# A tibble: 6 × 7\n  GEOID NAME   med_household_incomeE med_household_incomeM total_popE total_popM\n  &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt;                 &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 26001 Alcon…                 50295                  2243      10238         NA\n2 26003 Alger…                 55528                  2912       8866         NA\n3 26005 Alleg…                 75543                  2369     120189         NA\n4 26007 Alpen…                 49133                  2119      28911         NA\n5 26009 Antri…                 68850                  3115      23662         NA\n6 26011 Arena…                 53487                  2018      15031         NA\n# ℹ 1 more variable: county_data_2022_clean &lt;chr&gt;"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#data-quality-assessment",
    "href": "assignments/assignment_1/assignment1.html#data-quality-assessment",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\nMI_county_reliablility &lt;- county_data_2022 %&gt;%\n  mutate(\n    med_income_moe_pct = (med_household_incomeM / med_household_incomeE) * 100,\n    med_income_confi = case_when(\n      med_income_moe_pct &lt; 5 ~ \"High Confidence (&lt;5%)\",\n      med_income_moe_pct &gt; 5 & med_income_moe_pct &lt;10 ~ \"Moderate Confidence (5% - 10%)\",\n      med_income_moe_pct &gt; 10  ~ \"Low Confidence (&gt;10%)\"\n    )\n  )\n# Create a summary showing count of counties in each reliability category\nMI_reability_data &lt;- MI_county_reliablility %&gt;%\n  count(med_income_confi)%&gt;%\n  mutate(percentage = round(n/sum(n)*100, 2))\n\nkable(MI_reability_data,\n      caption = \"**MI Reliability Summary**\",\n      align = c(\"l\", \"c\", \"r\"),\n      digits = 2,\n      row.names = TRUE,\n      col.names = c(\"Confidence\", \"Count\", \"Percentage\")\n)\n\n\nMI Reliability Summary\n\n\n\nConfidence\nCount\nPercentage\n\n\n\n\n1\nHigh Confidence (&lt;5%)\n56\n67.47\n\n\n2\nLow Confidence (&gt;10%)\n2\n2.41\n\n\n3\nModerate Confidence (5% - 10%)\n25\n30.12\n\n\n\n\n# Hint: use count() and mutate() to add percentages"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#high-uncertainty-counties",
    "href": "assignments/assignment_1/assignment1.html#high-uncertainty-counties",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\ntop5_byMOE &lt;- MI_county_reliablility %&gt;% \n  arrange(desc(med_income_moe_pct))%&gt;% \n  slice(1:5)%&gt;% \n  select(county_data_2022_clean, med_household_incomeE, med_household_incomeM, med_income_moe_pct, med_income_confi) \n\nkable(top5_byMOE, \n      caption = \"**Top 5 Counties by MOE Percentage**\", \n      align = c(\"l\", \"c\", \"c\", \"c\", \"r\"),\n      digits = 2, \n      row.names = TRUE, \n      col.names = c(\"County\", \"Med. Household Income\", \"Income MOE\", \"MOE %\", \"Confidence Intervals\"))\n\n\nTop 5 Counties by MOE Percentage\n\n\n\n\n\n\n\n\n\n\n\nCounty\nMed. Household Income\nIncome MOE\nMOE %\nConfidence Intervals\n\n\n\n\n1\nKeweenaw\n55560\n7301\n13.14\nLow Confidence (&gt;10%)\n\n\n2\nSchoolcraft\n55071\n6328\n11.49\nLow Confidence (&gt;10%)\n\n\n3\nGogebic\n47913\n4766\n9.95\nModerate Confidence (5% - 10%)\n\n\n4\nOtsego\n62865\n5910\n9.40\nModerate Confidence (5% - 10%)\n\n\n5\nMontmorency\n46345\n3796\n8.19\nModerate Confidence (5% - 10%)\n\n\n\n\n# Format as table with kable() - include appropriate column names and caption\n\nData Quality Commentary:\nThese are the top 5 counties in Michigan with the highest MOE. Only Keweenaw and Schoolcraft maintain the lowest confidence intervals, probably due to being some of the smallest counties in the state by population (therefore, presumably, little regard is afforded to them and the residents)."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#focus-area-selection",
    "href": "assignments/assignment_1/assignment1.html#focus-area-selection",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\nselected_counties = top5_byMOE %&gt;%\n  filter(med_household_incomeE == 55071 | med_household_incomeE == 47913)\n\n      \n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\nselected_counties %&gt;%\n  select(county_data_2022_clean, med_household_incomeE, med_income_moe_pct, med_income_confi) %&gt;%\n  kable(\n        caption = \"**Schoolcraft and Gogebic County Income Demographics**\",\n        align = c(\"l\", \"c\", \"c\", \"r\"),\n        row.names = TRUE,\n        col.names = c(\"County Name\", \"Median Income\", \"MOE (%)\", \"Reliability Category\")\n  )\n\n\nSchoolcraft and Gogebic County Income Demographics\n\n\n\n\n\n\n\n\n\n\nCounty Name\nMedian Income\nMOE (%)\nReliability Category\n\n\n\n\n1\nSchoolcraft\n55071\n11.490621\nLow Confidence (&gt;10%)\n\n\n2\nGogebic\n47913\n9.947196\nModerate Confidence (5% - 10%)\n\n\n\n\n\nComment on the output: Despite Schoolcraft having a higher median income, the MOE surpasses the &gt;10% threshold and therefore falls into the Low Confidence category."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#tract-level-demographics",
    "href": "assignments/assignment_1/assignment1.html#tract-level-demographics",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\nrace_vars &lt;- get_acs(geography = \"tract\",\n                     survey = \"acs5\",\n                     variables = c(white = \"B03002_003\", \n                                   black = \"B03002_004\",\n                                   hisp_latinx = \"B03002_012\",\n                                   total_pop = \"B03002_001\"),\n                     year = 2022,\n                     state = \"MI\",\n                     county = c(\"153\", \"053\"), #Schoolcraft is 26153, Gogebic is 26053. \n                     output = \"wide\"\n)\n                     \n                     \n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\nrace_vars_percts = race_vars %&gt;%\n  mutate(perct_white = (whiteE/total_popE) * 100,\n         perct_black = (blackE/total_popE) * 100,\n         perct_latinx = (hisp_latinxE/total_popE) * 100,\n         tract_name = str_extract(NAME, \"Census Tract \\\\d+\"),\n         county_name = str_extract(NAME, \"[A-Za-z]+ County\" )\n  )\n\n# Add readable tract and county name columns using str_extract() or similar MUTATE()& Extract()"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#demographic-analysis",
    "href": "assignments/assignment_1/assignment1.html#demographic-analysis",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\nmax_latinx &lt;- race_vars_percts %&gt;%\n  arrange(desc(perct_latinx))%&gt;%\n  slice(1)\nkable(max_latinx,\n      caption = \"HELP\", \n      )\n\n\nHELP\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGEOID\nNAME\nwhiteE\nwhiteM\nblackE\nblackM\nhisp_latinxE\nhisp_latinxM\ntotal_popE\ntotal_popM\nperct_white\nperct_black\nperct_latinx\ntract_name\ncounty_name\n\n\n\n\n26053950600\nCensus Tract 9506; Gogebic County; Michigan\n2593\n183\n0\n11\n122\n50\n2817\n187\n92.04828\n0\n4.330848\nCensus Tract 9506\nGogebic County\n\n\n\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\navg_demo &lt;- race_vars_percts %&gt;%\n  group_by(county_name) %&gt;%\n  summarize(\n    ntracts    = n(),\n    white_avg  = mean(perct_white, na.rm = TRUE),\n    black_avg  = mean(perct_black, na.rm = TRUE),\n    latinx_avg = mean(perct_latinx, na.rm = TRUE)\n  )\n# Create a nicely formatted table of your results using kable()\nkable(avg_demo,\n      caption = \"Average Demographic Per County\",\n      align = c(\"l\", \"c\", \"c\", \"c\", \"r\"),\n      col.names = c(\"County Name\", \"No. of Tracts\", \"% White\", \"% Black\", \"% LatinX\"),\n      row.names = TRUE\n)\n\n\nAverage Demographic Per County\n\n\n\n\n\n\n\n\n\n\n\nCounty Name\nNo. of Tracts\n% White\n% Black\n% LatinX\n\n\n\n\n1\nGogebic County\n7\n90.86211\n1.7049326\n1.841025\n\n\n2\nSchoolcraft County\n4\n84.84429\n0.5191899\n1.504404"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment_1/assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\nmoe_gogegic_schoolcraft = race_vars_percts %&gt;%\n  mutate(\n  MOE_white = (whiteM/whiteE) * 100, \n  MOE_black = (blackM/blackE) * 100,\n  MOE_latinx = (hisp_latinxM/hisp_latinxE) * 100,\n  MOE_flag = ifelse(MOE_white &gt; 15 | MOE_black &gt; 15 |  MOE_latinx &gt; 15,\n  TRUE,\n  FALSE\n  )\n  )\n# Create summary statistics showing how many tracts have data quality issues\nmoe_summary_county &lt;- moe_gogegic_schoolcraft %&gt;%\n  group_by(county_name) %&gt;%\n  summarize(\n    total_tracts    = n(),\n    high_MOE_tracts = sum(MOE_flag, na.rm = TRUE),\n    pct_high_MOE    = round(100 * high_MOE_tracts / total_tracts, 1)\n  )\n\nkable (\n  moe_summary_county,\n  caption = \"**MOE Summary**\",\n  row.names = TRUE,\n  col.names = c(\"County\", \"Total Tracts\", \"High MOE Tracts\", \"Percent High MOE(%)\"),\n  align = c(\"l\",\"c\",\"c\",\"r\")\n)\n\n\nMOE Summary\n\n\n\n\n\n\n\n\n\n\nCounty\nTotal Tracts\nHigh MOE Tracts\nPercent High MOE(%)\n\n\n\n\n1\nGogebic County\n7\n7\n100\n\n\n2\nSchoolcraft County\n4\n4\n100"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#pattern-analysis",
    "href": "assignments/assignment_1/assignment1.html#pattern-analysis",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\nmoe_patterns &lt;- moe_gogegic_schoolcraft %&gt;%\n  group_by(county_name, MOE_flag) %&gt;%\n  summarize(\n    avg_pop     = mean(total_popE, na.rm = TRUE),\n    avg_white   = mean(perct_white, na.rm = TRUE),\n    avg_black   = mean(perct_black, na.rm = TRUE),\n    avg_latinx  = mean(perct_latinx, na.rm = TRUE),\n    n_tracts    = n()\n  )\n\nkable(moe_patterns,\n      caption = \"**Comparison of High-MOE vs Reliable Tracts**\",\n      align = c(\"l\", \"c\", \"c\", \"c\",\"c\",\"c\",\"r\"),\n      col.names = c(\"County\", \"Flag Status\", \"Population Average\", \"% White Avg\", \"% Black Avg\", \"% LatinX\", \"Tracts Quantity\"),\n      digits = 2\n)\n\n\nComparison of High-MOE vs Reliable Tracts\n\n\n\n\n\n\n\n\n\n\n\nCounty\nFlag Status\nPopulation Average\n% White Avg\n% Black Avg\n% LatinX\nTracts Quantity\n\n\n\n\nGogebic County\nTRUE\n2085.29\n90.86\n1.70\n1.84\n7\n\n\nSchoolcraft County\nTRUE\n2015.50\n84.84\n0.52\n1.50\n4\n\n\n\n\n\nPattern Analysis: Both counties have a population between 2000 - 3000, but Gogebic County has four more tracts available than Schoolcraft. It doesn’t seem to matter much because in previous evaluations, demographic results for white estimated, black estimated, and latinx estimated were 0, with an MOE of 11. The sample size is just too small. I am sincerely not surprised that both of these counties are flagged for having high MOEs."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment_1/assignment1.html#analysis-integration-and-professional-summary",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\nSome systematic patterns across all analyses is that the minority population is immensely small compared to the overwhelming majority of 90% white. The MOE has been consistently higher, which has led to some tracts within both counties to have a population estimate of 0, indicating a small sample size.\nBoth the black and LatinX communities face algorithmic biased simply because their population is so small, meaning there is likely to be a greater margin of error."
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#specific-recommendations",
    "href": "assignments/assignment_1/assignment1.html#specific-recommendations",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\nsum_table = selected_counties %&gt;%\n  select(\n    county_name = county_data_2022_clean,\n    med_inc = med_household_incomeE,\n    moe_pct = med_income_moe_pct,\n    conf_interval = med_income_confi\n  ) %&gt;%\n  \n\n  mutate(\n    recs = case_when(\n      moe_pct &lt; 5 ~ \"Safe for algorithmic decisions\",\n      moe_pct &lt; 10 ~ \"Use with caution - monitor outcomes\",\n      TRUE ~ \"Requires manual review or additional data\"\n    )\n  )\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\nkable(sum_table,\n      caption = \"**Algorithm Recommendations**\",\n      col.names = c(\"County\", \"Median Income\", \"MOE (%)\", \"Confidence Interval\", \"Recommendation\"),\n      digits = 2,\n      align = c(\"l\", \"c\", \"c\", \"c\", \"r\"),\n      \n)\n\n\nAlgorithm Recommendations\n\n\n\n\n\n\n\n\n\nCounty\nMedian Income\nMOE (%)\nConfidence Interval\nRecommendation\n\n\n\n\nSchoolcraft\n55071\n11.49\nLow Confidence (&gt;10%)\nRequires manual review or additional data\n\n\nGogebic\n47913\n9.95\nModerate Confidence (5% - 10%)\nUse with caution - monitor outcomes\n\n\n\n\n# Format as a professional table with kable()\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: [List counties with high confidence data and explain why they’re appropriate]\nCounties requiring additional oversight: [List counties with moderate confidence data and describe what kind of monitoring would be needed]\nCounties needing alternative approaches: [List counties with low confidence data and suggest specific alternatives - manual review, additional surveys, etc.]"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#questions-for-further-investigation",
    "href": "assignments/assignment_1/assignment1.html#questions-for-further-investigation",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n[List 2-3 questions that your analysis raised that you’d like to explore further in future assignments. Consider questions about spatial patterns, time trends, or other demographic factors.]"
  },
  {
    "objectID": "assignments/assignment_1/assignment1.html#submission-checklist",
    "href": "assignments/assignment_1/assignment1.html#submission-checklist",
    "title": "Lab 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML"
  },
  {
    "objectID": "assignments/assignment_2/assignment2.html",
    "href": "assignments/assignment_2/assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "assignments/assignment_2/assignment2.html#assignment-overview",
    "href": "assignments/assignment_2/assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "assignments/assignment_2/assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "assignments/assignment_2/assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data: - Pennsylvania county boundaries - Pennsylvania hospitals (from lecture data) - Pennsylvania census tracts\nYour Task:\n\n# Load required packages\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(tigris)\n\n# Load spatial data\ncounties &lt;- st_read(\"E:/Upenn_course/policy/MUSA-5080-Fall-2025/MUSA-5080-Fall-2025/MUSA-5080-Fall-2025/lectures/week-04/data/Pennsylvania_County_Boundaries.shp\")\n\nReading layer `Pennsylvania_County_Boundaries' from data source \n  `E:\\Upenn_course\\policy\\MUSA-5080-Fall-2025\\MUSA-5080-Fall-2025\\MUSA-5080-Fall-2025\\lectures\\week-04\\data\\Pennsylvania_County_Boundaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 67 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8963377 ymin: 4825316 xmax: -8314404 ymax: 5201413\nProjected CRS: WGS 84 / Pseudo-Mercator\n\ncensus_api_key(\"940dffa67b928a0518accaf8839aa7b4762b11ab\")\ncensus_tracts &lt;- tracts(state = \"PA\", cb = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\nhospitals &lt;- st_read(\"E:/Upenn_course/policy/MUSA-5080-Fall-2025/MUSA-5080-Fall-2025/MUSA-5080-Fall-2025/lectures/week-04/data/hospitals.geojson\")\n\nReading layer `hospitals' from data source \n  `E:\\Upenn_course\\policy\\MUSA-5080-Fall-2025\\MUSA-5080-Fall-2025\\MUSA-5080-Fall-2025\\lectures\\week-04\\data\\hospitals.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 223 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.49621 ymin: 39.75163 xmax: -74.86704 ymax: 42.13403\nGeodetic CRS:  WGS 84\n\n# 3. Pennsylvania census tracts\n# Check that all data loaded correctly\nhead(counties)\n\nSimple feature collection with 6 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8905670 ymin: 4862594 xmax: -8317930 ymax: 5161279\nProjected CRS: WGS 84 / Pseudo-Mercator\n  OBJECTID MSLINK COUNTY_NAM COUNTY_NUM FIPS_COUNT COUNTY_ARE COUNTY_PER\n1      336     46 MONTGOMERY         46        091       &lt;NA&gt;       &lt;NA&gt;\n2      337      8   BRADFORD         08        015       &lt;NA&gt;       &lt;NA&gt;\n3      338      9      BUCKS         09        017       &lt;NA&gt;       &lt;NA&gt;\n4      339     58      TIOGA         58        117       &lt;NA&gt;       &lt;NA&gt;\n5      340     59      UNION         59        119       &lt;NA&gt;       &lt;NA&gt;\n6      341     60    VENANGO         60        121       &lt;NA&gt;       &lt;NA&gt;\n  NUMERIC_LA COUNTY_N_1 AREA_SQ_MI SOUND SPREAD_SHE IMAGE_NAME NOTE_FILE VIDEO\n1          5         46   487.4271  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n2          2          8  1161.3379  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n3          5          9   622.0836  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n4          2         58  1137.2480  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n5          2         59   319.1893  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n6          3         60   683.3676  &lt;NA&gt;       &lt;NA&gt;   poll.bmp      &lt;NA&gt;  &lt;NA&gt;\n  DISTRICT_N PA_CTY_COD MAINT_CTY_ DISTRICT_O                       geometry\n1         06         46          4        6-4 MULTIPOLYGON (((-8398884 48...\n2         03         08          9        3-9 MULTIPOLYGON (((-8558633 51...\n3         06         09          1        6-1 MULTIPOLYGON (((-8367360 49...\n4         03         59          7        3-7 MULTIPOLYGON (((-8558633 51...\n5         03         60          8        3-8 MULTIPOLYGON (((-8562865 49...\n6         01         61          5        1-5 MULTIPOLYGON (((-8870781 50...\n\nhead(hospitals)\n\nSimple feature collection with 6 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.27907 ymin: 39.80913 xmax: -75.17005 ymax: 40.24273\nGeodetic CRS:  WGS 84\n                 CHIEF_EXEC              CHIEF_EX_1\n1             Peter J Adamo               President\n2          Autumn DeShields Chief Executive Officer\n3              Shawn Parekh Chief Executive Officer\n4               DIANE HRITZ Chief Executive Officer\n5            Tim Harclerode Chief Executive Officer\n6 Richard McLaughlin MD MBA Chief Executive Officer\n                      FACILITY_U LONGITUDE       COUNTY\n1   https://www.phhealthcare.org -79.91131   Washington\n2      https://www.malvernbh.com -75.17005 Philadelphia\n3 https://roxboroughmemorial.com -75.20963 Philadelphia\n4     https://www.ashospital.net -80.27907   Washington\n5      https://www.conemaugh.org -79.02513     Somerset\n6        https://towerhealth.org -75.61213   Montgomery\n                               FACILITY_N                         STREET\n1               Penn Highlands Mon Valley         1163 Country Club Road\n2               MALVERN BEHAVIORAL HEALTH 1930 South Broad Street Unit 4\n3            Roxborough Memorial Hospital              5800 Ridge Avenue\n4              ADVANCED SURGICAL HOSPITAL       100 TRICH DRIVE\\nSUITE 1\n5 DLP Conemaugh Meyersdale Medical Center             200 Hospital Drive\n6                 Pottstown Hospital, LLC          1600 East High Street\n    CITY_OR_BO LATITUDE   TELEPHONE_ ZIP_CODE                   geometry\n1  Monongahela 40.18193 724-258-1000    15063 POINT (-79.91131 40.18193)\n2 Philadelphia 39.92619 610-480-8919    19145  POINT (-75.17005 39.9262)\n3 Philadelphia 40.02869 215-483-9900    19128 POINT (-75.20963 40.02869)\n4   WASHINGTON 40.15655   7248840710    15301 POINT (-80.27907 40.15655)\n5   Meyersdale 39.80913 814-634-5911    15552 POINT (-79.02513 39.80913)\n6    Pottstown 40.24273   6103277000    19464 POINT (-75.61213 40.24273)\n\nhead(census_tracts)\n\nSimple feature collection with 6 features and 13 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -78.42478 ymin: 39.79351 xmax: -75.93766 ymax: 40.54328\nGeodetic CRS:  NAD83\n  STATEFP COUNTYFP TRACTCE              GEOIDFQ       GEOID   NAME\n1      42      001  031101 1400000US42001031101 42001031101 311.01\n2      42      013  100400 1400000US42013100400 42013100400   1004\n3      42      013  100500 1400000US42013100500 42013100500   1005\n4      42      013  100800 1400000US42013100800 42013100800   1008\n5      42      013  101900 1400000US42013101900 42013101900   1019\n6      42      011  011200 1400000US42011011200 42011011200    112\n             NAMELSAD STUSPS   NAMELSADCO   STATE_NAME LSAD   ALAND AWATER\n1 Census Tract 311.01     PA Adams County Pennsylvania   CT 3043185      0\n2   Census Tract 1004     PA Blair County Pennsylvania   CT  993724      0\n3   Census Tract 1005     PA Blair County Pennsylvania   CT 1130204      0\n4   Census Tract 1008     PA Blair County Pennsylvania   CT  996553      0\n5   Census Tract 1019     PA Blair County Pennsylvania   CT  573726      0\n6    Census Tract 112     PA Berks County Pennsylvania   CT 1539365   9308\n                        geometry\n1 MULTIPOLYGON (((-77.03108 3...\n2 MULTIPOLYGON (((-78.42478 4...\n3 MULTIPOLYGON (((-78.41661 4...\n4 MULTIPOLYGON (((-78.41067 4...\n5 MULTIPOLYGON (((-78.40836 4...\n6 MULTIPOLYGON (((-75.95433 4...\n\n\nQuestions to answer: - How many hospitals are in your dataset? There are 223 hospital in my dataset. - How many census tracts? 3345 - What coordinate reference system is each dataset in? NAD 83\n\n\n\nStep 2: Get Demographic Data\nUse tidycensus to download tract-level demographic data for Pennsylvania.\nRequired variables: - Total population - Median household income - Population 65 years and over (you may need to sum multiple age categories)\nYour Task:\n\n# Get demographic data from ACS\npa_demo &lt;- get_acs(\n  geography = \"tract\",\n  state = \"PA\",\n  variables = c(\n    total_pop = \"B01003_001\",       # 总人口\n    median_income = \"B19013_001\",   # 家庭收入中位数\n    age_65_66 = \"B01001_020\",       # 男 65-66岁\n    age_67_69 = \"B01001_021\",       # 男 67-69岁\n    age_70_74 = \"B01001_022\",       # 男 70-74岁\n    age_75_79 = \"B01001_023\",       # 男 75-79岁\n    age_80_84 = \"B01001_024\",       # 男 80-84岁\n    age_85_plus = \"B01001_025\",     # 男 85岁+\n    age_65_66_f = \"B01001_044\",     # 女 65-66岁\n    age_67_69_f = \"B01001_045\",     # 女 67-69岁\n    age_70_74_f = \"B01001_046\",     # 女 70-74岁\n    age_75_79_f = \"B01001_047\",     # 女 75-79岁\n    age_80_84_f = \"B01001_048\",     # 女 80-84岁\n    age_85_plus_f = \"B01001_049\"    # 女 85岁+\n  ),\n  geometry = TRUE,\n  year = 2021\n)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n# Step 2: Summarize population 65+ ----\npa_demo_clean &lt;- pa_demo |&gt;\n  select(GEOID, variable, estimate) |&gt;\n  pivot_wider(names_from = variable, values_from = estimate) |&gt;\n  mutate(\n    pop_65_over = age_65_66 + age_67_69 + age_70_74 + age_75_79 +\n                  age_80_84 + age_85_plus + age_65_66_f + age_67_69_f +\n                  age_70_74_f + age_75_79_f + age_80_84_f + age_85_plus_f\n  ) |&gt;\n  select(GEOID, total_pop, median_income, pop_65_over)\n\n# 查看结果\nhead(pa_demo_clean)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -80.08415 ymin: 39.92397 xmax: -75.09795 ymax: 40.48008\nGeodetic CRS:  NAD83\n# A tibble: 6 × 5\n  GEOID       total_pop median_income pop_65_over                       geometry\n  &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;             &lt;MULTIPOLYGON [°]&gt;\n1 42017104204      5256        112691         487 (((-75.18818 40.36427, -75.16…\n2 42101006600      3536         27049         452 (((-75.23699 39.92782, -75.23…\n3 42101018802      4200         31523          48 (((-75.10644 40.00032, -75.10…\n4 42017102600      2166         63393         626 (((-75.33392 40.33027, -75.33…\n5 42003462600      3637         44315         535 (((-80.08415 40.47571, -80.08…\n6 42101011100      3829         43235         422 (((-75.23017 39.97879, -75.22…\n\n# Join to tract boundaries\npa_tracts &lt;- tracts(state = \"PA\", cb = TRUE)\n\n# ✅ 正确写法：去掉右侧的几何列再 join\npa_joined &lt;- pa_tracts %&gt;%\n  left_join(st_drop_geometry(pa_demo_clean), by = \"GEOID\")\n\nQuestions to answer: - What year of ACS data are you using? 2021 - How many tracts have missing income data? 66\n- What is the median income across all PA census tracts? 65195.5\n\n\n\nStep 3: Define Vulnerable Populations\nIdentify census tracts with vulnerable populations based on TWO criteria: 1. Low median household income (choose an appropriate threshold) 2. Significant elderly population (choose an appropriate threshold)\nYour Task:\n\n# Filter for vulnerable tracts based on your criteria\npa_demo_clean &lt;- pa_demo_clean %&gt;%\n  mutate(\n    pct_elderly = (pop_65_over / total_pop) * 100\n  )\n\nincome_threshold &lt;- median(pa_demo_clean$median_income, na.rm = TRUE) * 0.8  # 比全州中位数低 20%\nelderly_threshold &lt;- 20\n\nvulnerable_tracts &lt;- pa_demo_clean %&gt;%\n  filter(median_income &lt; income_threshold & pct_elderly &gt; elderly_threshold)\n\n\nvulnerable_tracts &lt;- pa_demo_clean %&gt;%\n  filter(median_income &lt; income_threshold & pct_elderly &gt; elderly_threshold)\n\nnrow(vulnerable_tracts)\n\n[1] 278\n\nhead(vulnerable_tracts)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -79.89962 ymin: 40.04802 xmax: -75.03866 ymax: 40.86881\nGeodetic CRS:  NAD83\n# A tibble: 6 × 6\n  GEOID       total_pop median_income pop_65_over                       geometry\n  &lt;chr&gt;           &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;             &lt;MULTIPOLYGON [°]&gt;\n1 42101033300      3790         49505        1210 (((-75.0515 40.05046, -75.050…\n2 42087960900      2327         33750         505 (((-77.57825 40.59525, -77.57…\n3 42011001900      1833         19336         434 (((-75.91819 40.33353, -75.91…\n4 42077001900      4414         34297        1301 (((-75.50088 40.61841, -75.49…\n5 42003515300      1553         25833         397 (((-79.89887 40.41885, -79.89…\n6 42097082100      2463         29342         549 (((-76.8015 40.8547, -76.7978…\n# ℹ 1 more variable: pct_elderly &lt;dbl&gt;\n\n\nQuestions to answer: - What income threshold did you choose and why? I chose an income threshold equal to 80% of the statewide median household income (approximately $54,000) to identify tracts that fall substantially below the state’s overall income level. - What elderly population threshold did you choose and why? I defined tracts with more than 20% of residents aged 65 and over as having a significant elderly population, since this represents roughly the top quartile of tracts in Pennsylvania in terms of elderly share. - How many tracts meet your vulnerability criteria? - What percentage of PA census tracts are considered vulnerable by your definition? These tracts account for approximately 6.7% of all Pennsylvania census tracts, according to my criteria.\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\nYour Task:\n\nlibrary(sf)\nlibrary(tidyverse)\n\nvul_proj &lt;- st_transform(vulnerable_tracts, 26918)\nhosp_proj &lt;- st_transform(hospitals, 26918)\n\ntract_centroids &lt;- st_centroid(vul_proj)\ndist_matrix &lt;- st_distance(tract_centroids, hosp_proj)\n\nnearest_dist_m &lt;- apply(dist_matrix, 1, min)\n\nvul_proj &lt;- vul_proj %&gt;%\n  mutate(\n    dist_to_hosp_m = as.numeric(nearest_dist_m),\n    dist_to_hosp_mi = dist_to_hosp_m / 1609.34\n  )\n\navg_dist &lt;- mean(vul_proj$dist_to_hosp_mi, na.rm = TRUE)\nmax_dist &lt;- max(vul_proj$dist_to_hosp_mi, na.rm = TRUE)\nover15 &lt;- sum(vul_proj$dist_to_hosp_mi &gt; 15)\n\navg_dist\n\n[1] 5.151893\n\nmax_dist\n\n[1] 27.78481\n\nover15\n\n[1] 16\n\n\nRequirements: - Use an appropriate projected coordinate system for Pennsylvania - Calculate distances in miles - Explain why you chose your projection\nQuestions to answer: - What is the average distance to the nearest hospital for vulnerable tracts? ≈ 5.8 miles - What is the maximum distance? ≈ 27.4 miles - How many vulnerable tracts are more than 15 miles from the nearest hospital 12 tracts\n\n\n\nStep 5: Identify Underserved Areas\nDefine “underserved” as vulnerable tracts that are more than 15 miles from the nearest hospital.\nYour Task:\n\n# Create underserved variable\nvul_proj &lt;- vul_proj %&gt;%\n  mutate(\n    underserved = dist_to_hosp_mi &gt; 15\n  )\n\ntable(vul_proj$underserved)\n\n\nFALSE  TRUE \n  262    16 \n\n\nQuestions to answer: - How many tracts are underserved? 16\n\nWhat percentage of vulnerable tracts are underserved? 5.76%\nDoes this surprise you? Why or why not? In my opinion I feel confused about the distance we’ve defined as a undersevred distance.It would be good if we use 10 miles to the defenition of “undersevrved”.But only about 5.8% of vulnerable census tracts are located more than 15 miles from the nearest hospital, indicating that approximately 94%–95% of vulnerable areas are relatively close to hospital facilities.\n\n\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\nYour Task:\n\nvul_proj &lt;- st_transform(vul_proj, st_crs(counties))\ncounties &lt;- st_transform(counties, st_crs(vul_proj))\n\ntracts_with_county &lt;- st_join(vul_proj, counties, join = st_intersects)\n\n\n# Aggregate statistics by county\n\ncounty_summary &lt;- tracts_with_county %&gt;%\n  st_drop_geometry() %&gt;%  # 移除几何信息以便聚合\n  group_by(COUNTY_NAME = COUNTY_NAM) %&gt;%  # 如果你的字段叫 COUNTYNAME 或 NAME，请根据实际修改\n  summarise(\n    total_tracts = n(),\n    vulnerable_tracts = sum(!is.na(median_income)),  # 或根据你定义的vulnerable集调整\n    underserved_tracts = sum(underserved, na.rm = TRUE),\n    pct_underserved = (underserved_tracts / total_tracts) * 100,\n    avg_dist_miles = mean(dist_to_hosp_mi, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(pct_underserved))\n\nhead(county_summary)\n\n# A tibble: 6 × 6\n  COUNTY_NAME total_tracts vulnerable_tracts underserved_tracts pct_underserved\n  &lt;chr&gt;              &lt;int&gt;             &lt;int&gt;              &lt;int&gt;           &lt;dbl&gt;\n1 PERRY                  2                 2                  2           100  \n2 CLINTON                3                 3                  2            66.7\n3 SULLIVAN               3                 3                  2            66.7\n4 BRADFORD               4                 4                  2            50  \n5 CAMERON                6                 6                  3            50  \n6 COLUMBIA               2                 2                  1            50  \n# ℹ 1 more variable: avg_dist_miles &lt;dbl&gt;\n\n\nRequired county-level statistics: - Number of vulnerable tracts - Number of underserved tracts\n- Percentage of vulnerable tracts that are underserved - Average distance to nearest hospital for vulnerable tracts - Total vulnerable population\nQuestions to answer: - Which 5 counties have the highest percentage of underserved vulnerable tracts? 1.PERRY 2.CLINTON 3.SULLIVAN 4.BRADFORD 5.ACMERON\n\nWhich counties have the most vulnerable people living far from hospitals?\nAre there any patterns in where underserved counties are located?\n\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\nYour Task:\n\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(scales)\n\ncounty_summary &lt;- tracts_with_county %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAME = COUNTY_NAM) %&gt;%\n  summarise(\n    num_vulnerable_tracts = n(),\n    num_underserved_tracts = sum(underserved, na.rm = TRUE),\n    pct_underserved = (num_underserved_tracts / num_vulnerable_tracts) * 100,\n    avg_dist_miles = mean(dist_to_hosp_mi, na.rm = TRUE),\n    total_vulnerable_pop = sum(total_pop, na.rm = TRUE),\n    underserved_pop = sum(if_else(underserved, total_pop, 0), na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(pct_underserved))\n\ncounty_summary %&gt;%\n  mutate(\n    `# Vulnerable Tracts` = num_vulnerable_tracts,\n    `# Underserved Tracts` = num_underserved_tracts,\n    `% Underserved` = percent(pct_underserved / 100, accuracy = 0.1),\n    `Avg. Distance (mi)` = round(avg_dist_miles, 2),\n    `Total Vulnerable Pop.` = comma(total_vulnerable_pop),\n    `Underserved Pop.` = comma(underserved_pop)\n  ) %&gt;%\n  select(\n    County = COUNTY_NAME,\n    `# Vulnerable Tracts`,\n    `# Underserved Tracts`,\n    `% Underserved`,\n    `Avg. Distance (mi)`,\n    `Total Vulnerable Pop.`,\n    `Underserved Pop.`\n  ) %&gt;%\n  kable(\n    caption = \"County-Level Summary of Vulnerable and Underserved Census Tracts in Pennsylvania\"\n  )\n\n\nCounty-Level Summary of Vulnerable and Underserved Census Tracts in Pennsylvania\n\n\n\n\n\n\n\n\n\n\n\nCounty\n# Vulnerable Tracts\n# Underserved Tracts\n% Underserved\nAvg. Distance (mi)\nTotal Vulnerable Pop.\nUnderserved Pop.\n\n\n\n\nPERRY\n2\n2\n100.0%\n17.53\n5,815\n5,815\n\n\nCLINTON\n3\n2\n66.7%\n13.84\n7,750\n4,615\n\n\nSULLIVAN\n3\n2\n66.7%\n18.28\n6,949\n3,031\n\n\nBRADFORD\n4\n2\n50.0%\n14.14\n14,748\n7,562\n\n\nCAMERON\n6\n3\n50.0%\n14.09\n13,466\n6,763\n\n\nCOLUMBIA\n2\n1\n50.0%\n9.45\n5,897\n970\n\n\nDAUPHIN\n2\n1\n50.0%\n10.01\n5,838\n4,028\n\n\nJUNIATA\n2\n1\n50.0%\n12.56\n5,461\n1,787\n\n\nELK\n8\n3\n37.5%\n12.71\n19,260\n8,045\n\n\nCLEARFIELD\n11\n4\n36.4%\n13.47\n36,592\n10,359\n\n\nCENTRE\n3\n1\n33.3%\n15.08\n11,735\n2,167\n\n\nFOREST\n3\n1\n33.3%\n14.09\n6,031\n2,603\n\n\nPOTTER\n6\n2\n33.3%\n10.29\n18,675\n4,615\n\n\nBEDFORD\n4\n1\n25.0%\n11.45\n17,181\n4,021\n\n\nCLARION\n8\n2\n25.0%\n12.40\n22,104\n7,149\n\n\nFRANKLIN\n4\n1\n25.0%\n5.61\n14,268\n1,787\n\n\nHUNTINGDON\n4\n1\n25.0%\n11.41\n12,266\n1,787\n\n\nMIFFLIN\n4\n1\n25.0%\n10.28\n10,268\n1,787\n\n\nMONROE\n4\n1\n25.0%\n12.07\n8,388\n1,134\n\n\nCRAWFORD\n6\n1\n16.7%\n8.37\n16,001\n2,661\n\n\nJEFFERSON\n6\n1\n16.7%\n9.40\n16,311\n4,546\n\n\nLYCOMING\n6\n1\n16.7%\n8.27\n21,547\n970\n\n\nTIOGA\n6\n1\n16.7%\n10.30\n21,484\n1,953\n\n\nVENANGO\n6\n1\n16.7%\n10.49\n14,597\n2,603\n\n\nMCKEAN\n7\n1\n14.3%\n10.04\n16,897\n2,662\n\n\nARMSTRONG\n8\n1\n12.5%\n10.74\n23,310\n4,546\n\n\nSOMERSET\n8\n1\n12.5%\n9.24\n24,350\n4,021\n\n\nINDIANA\n9\n1\n11.1%\n7.36\n31,370\n4,546\n\n\nWARREN\n9\n1\n11.1%\n7.01\n27,976\n2,603\n\n\nNORTHUMBERLAND\n10\n1\n10.0%\n11.66\n33,370\n4,028\n\n\nLUZERNE\n14\n1\n7.1%\n5.30\n35,497\n970\n\n\nALLEGHENY\n47\n0\n0.0%\n2.45\n109,061\n0\n\n\nBEAVER\n8\n0\n0.0%\n4.34\n17,930\n0\n\n\nBERKS\n2\n0\n0.0%\n2.95\n4,073\n0\n\n\nBLAIR\n5\n0\n0.0%\n2.97\n14,939\n0\n\n\nBUTLER\n3\n0\n0.0%\n8.60\n9,545\n0\n\n\nCAMBRIA\n14\n0\n0.0%\n6.35\n34,583\n0\n\n\nCARBON\n3\n0\n0.0%\n8.14\n7,325\n0\n\n\nCUMBERLAND\n2\n0\n0.0%\n1.64\n6,734\n0\n\n\nDELAWARE\n7\n0\n0.0%\n1.27\n26,294\n0\n\n\nERIE\n7\n0\n0.0%\n2.36\n20,784\n0\n\n\nFAYETTE\n16\n0\n0.0%\n4.05\n50,911\n0\n\n\nFULTON\n1\n0\n0.0%\n10.64\n3,354\n0\n\n\nLACKAWANNA\n10\n0\n0.0%\n4.01\n29,208\n0\n\n\nLANCASTER\n2\n0\n0.0%\n6.60\n8,186\n0\n\n\nLAWRENCE\n5\n0\n0.0%\n5.80\n12,399\n0\n\n\nLEBANON\n2\n0\n0.0%\n3.61\n8,456\n0\n\n\nLEHIGH\n4\n0\n0.0%\n2.09\n11,383\n0\n\n\nMERCER\n5\n0\n0.0%\n4.02\n16,963\n0\n\n\nMONTGOMERY\n3\n0\n0.0%\n1.12\n7,081\n0\n\n\nMONTOUR\n1\n0\n0.0%\n0.56\n4,282\n0\n\n\nNORTHAMPTON\n4\n0\n0.0%\n2.41\n11,585\n0\n\n\nPHILADELPHIA\n21\n0\n0.0%\n1.00\n73,982\n0\n\n\nPIKE\n2\n0\n0.0%\n14.14\n3,903\n0\n\n\nSCHUYLKILL\n6\n0\n0.0%\n4.38\n17,988\n0\n\n\nSUSQUEHANNA\n1\n0\n0.0%\n5.79\n1,658\n0\n\n\nUNION\n3\n0\n0.0%\n4.15\n16,364\n0\n\n\nWASHINGTON\n5\n0\n0.0%\n4.66\n11,865\n0\n\n\nWAYNE\n4\n0\n0.0%\n8.73\n9,971\n0\n\n\nWESTMORELAND\n22\n0\n0.0%\n4.05\n54,779\n0\n\n\nYORK\n4\n0\n0.0%\n1.44\n11,785\n0\n\n\n\n\n\nRequirements: - Use knitr::kable() or similar for formatting - Include descriptive column names - Format numbers appropriately (commas for population, percentages, etc.) - Add an informative caption - Sort by priority (you decide the metric)"
  },
  {
    "objectID": "assignments/assignment_2/assignment2.html#part-2-comprehensive-visualization",
    "href": "assignments/assignment_2/assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\nYour Task:\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(viridis)    \nlibrary(scales)      \n\ncounties_proj &lt;- st_transform(counties, st_crs(vul_proj))\nhospitals_proj &lt;- st_transform(hospitals, st_crs(counties_proj))\n\ncounty_map &lt;- counties_proj %&gt;%\n  left_join(county_summary, by = c(\"COUNTY_NAM\" = \"COUNTY_NAME\"))\n\nggplot() +\n  \n  geom_sf(data = county_map,\n          aes(fill = pct_underserved),\n          color = \"white\", size = 0.3) +\n  \n  geom_sf(data = hospitals_proj,\n          shape = 21, color = \"black\", fill = \"red\", size = 2, alpha = 0.8)+\n\n  scale_fill_viridis(\n    name = \"% Underserved Vulnerable Tracts\",\n    option = \"magma\",\n    direction = -1,\n    labels = function(x) paste0(round(x, 1), \"%\")\n  ) +\n  \n   labs(\n    title = \"Healthcare Access Challenges in Pennsylvania\",\n    subtitle = \"Counties colored by percentage of vulnerable tracts that are underserved (&gt;15 miles from nearest hospital)\",\n    caption = \"Data sources: ACS 2021 (via tidycensus), hospital locations (PA DOH), analysis by Yanyang Chen\"\n  ) +\n\n   theme_void() +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 8),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, margin = margin(b = 8)),\n    plot.caption = element_text(size = 8, color = \"gray40\")\n  )\n\n\n\n\n\n\n\n\nRequirements: - Fill counties by percentage of vulnerable tracts that are underserved - Include hospital locations as points - Use an appropriate color scheme - Include clear title, subtitle, and caption - Use theme_void() or similar clean theme - Add a legend with formatted labels\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\nYour Task:\n\n# Create detailed tract-level map\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(ggplot2)\n\ntracts_proj &lt;- st_transform(vul_proj, st_crs(counties))\ncounties_proj &lt;- st_transform(counties, st_crs(tracts_proj))\nhospitals_proj &lt;- st_transform(hospitals, st_crs(tracts_proj))\n\nggplot() +\n  \n  geom_sf(data = counties_proj, fill = NA, color = \"gray60\", size = 0.3) +\n  \n  geom_sf(data = tracts_proj, aes(geometry = geometry),\n          fill = \"lightgray\", color = NA) +\n  \n  geom_sf(data = filter(tracts_proj, underserved == TRUE),\n          aes(geometry = geometry), fill = \"#d73027\", color = NA, alpha = 0.9) +\n  \n  geom_sf(data = hospitals_proj,\n          shape = 21, fill = \"yellow\", color = \"black\", size = 2, alpha = 0.9) +\n  \n  labs(\n    title = \"Map 2. Detailed Tract-Level Vulnerability in Pennsylvania\",\n    subtitle = \"Underserved vulnerable tracts (&gt;15 miles from nearest hospital) shown in red\",\n    caption = \"Data sources: ACS 2021 via tidycensus; Hospital data: PA Department of Health\"\n  ) +\n  \n  theme_void() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\", color = \"black\"),\n    plot.subtitle = element_text(size = 10, color = \"gray30\"),\n    plot.caption = element_text(size = 8, color = \"gray40\"),\n    panel.background = element_rect(fill = \"white\", color = NA)\n  )\n\n\n\n\n\n\n\n\nRequirements: - Show underserved vulnerable tracts in a contrasting color - Include county boundaries for context - Show hospital locations - Use appropriate visual hierarchy (what should stand out?) - Include informative title and subtitle\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\nYour Task:\n\n# Create distribution visualization\nlibrary(ggplot2)\nlibrary(scales)\n\n# Step: Distribution of distances for vulnerable tracts ----\nggplot(vul_proj, aes(x = dist_to_hosp_mi)) +\n  # 直方图部分\n  geom_histogram(aes(y = after_stat(density)), \n                 bins = 30, fill = \"#3182bd\", color = \"white\", alpha = 0.7) +\n  # 密度曲线部分\n  geom_density(color = \"#de2d26\", size = 1, alpha = 0.6) +\n  \n  # 标题与坐标轴\n  labs(\n    title = \"Distribution of Distances to Nearest Hospital\",\n    subtitle = \"For vulnerable census tracts across Pennsylvania\",\n    x = \"Distance to Nearest Hospital (miles)\",\n    y = \"Density\",\n    caption = \"Data: ACS 2021 (via tidycensus) and PA DOH hospital locations\"\n  ) +\n  \n  # 美观格式\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, color = \"gray30\"),\n    plot.caption = element_text(size = 8, color = \"gray40\"),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nSuggested chart types: - Histogram or density plot of distances - Box plot comparing distances across regions - Bar chart of underserved tracts by county - Scatter plot of distance vs. vulnerable population size\nRequirements: - Clear axes labels with units - Appropriate title - Professional formatting - Brief interpretation (1-2 sentences as a caption or in text)"
  },
  {
    "objectID": "assignments/assignment_2/assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "assignments/assignment_2/assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\nChoose your own additional spatial dataset and conduct a supplementary analysis.\n\nChallenge Options\nChoose ONE of the following challenge exercises, or propose your own research question using OpenDataPhilly data (https://opendataphilly.org/datasets/).\nNote these are just loose suggestions to spark ideas - follow or make your own as the data permits and as your ideas evolve. This analysis should include bringing in your own dataset, ensuring the projection/CRS of your layers align and are appropriate for the analysis (not lat/long or geodetic coordinate systems). The analysis portion should include some combination of spatial and attribute operations to answer a relatively straightforward question\n\n\nEducation & Youth Services\nOption A: Educational Desert Analysis - Data: Schools, Libraries, Recreation Centers, Census tracts (child population) - Question: “Which neighborhoods lack adequate educational infrastructure for children?” - Operations: Buffer schools/libraries (0.5 mile walking distance), identify coverage gaps, overlay with child population density - Policy relevance: School district planning, library placement, after-school program siting\nOption B: School Safety Zones - Data: Schools, Crime Incidents, Bike Network - Question: “Are school zones safe for walking/biking, or are they crime hotspots?” - Operations: Buffer schools (1000ft safety zone), spatial join with crime incidents, assess bike infrastructure coverage - Policy relevance: Safe Routes to School programs, crossing guard placement\n\n\n\nEnvironmental Justice\nOption C: Green Space Equity - Data: Parks, Street Trees, Census tracts (race/income demographics) - Question: “Do low-income and minority neighborhoods have equitable access to green space?” - Operations: Buffer parks (10-minute walk = 0.5 mile), calculate tree canopy or park acreage per capita, compare by demographics - Policy relevance: Climate resilience, environmental justice, urban forestry investment\n\n\n\nPublic Safety & Justice\nOption D: Crime & Community Resources - Data: Crime Incidents, Recreation Centers, Libraries, Street Lights - Question: “Are high-crime areas underserved by community resources?” - Operations: Aggregate crime counts to census tracts or neighborhoods, count community resources per area, spatial correlation analysis - Policy relevance: Community investment, violence prevention strategies\n\n\n\nInfrastructure & Services\nOption E: Polling Place Accessibility - Data: Polling Places, SEPTA stops, Census tracts (elderly population, disability rates) - Question: “Are polling places accessible for elderly and disabled voters?” - Operations: Buffer polling places and transit stops, identify vulnerable populations, find areas lacking access - Policy relevance: Voting rights, election infrastructure, ADA compliance\n\n\n\nHealth & Wellness\nOption F: Recreation & Population Health - Data: Recreation Centers, Playgrounds, Parks, Census tracts (demographics) - Question: “Is lack of recreation access associated with vulnerable populations?” - Operations: Calculate recreation facilities per capita by neighborhood, buffer facilities for walking access, overlay with demographic indicators - Policy relevance: Public health investment, recreation programming, obesity prevention\n\n\n\nEmergency Services\nOption G: EMS Response Coverage - Data: Fire Stations, EMS stations, Population density, High-rise buildings - Question: “Are population-dense areas adequately covered by emergency services?” - Operations: Create service area buffers (5-minute drive = ~2 miles), assess population coverage, identify gaps in high-density areas - Policy relevance: Emergency preparedness, station siting decisions\n\n\n\nArts & Culture\nOption H: Cultural Asset Distribution - Data: Public Art, Museums, Historic sites/markers, Neighborhoods - Question: “Do all neighborhoods have equitable access to cultural amenities?” - Operations: Count cultural assets per neighborhood, normalize by population, compare distribution across demographic groups - Policy relevance: Cultural equity, tourism, quality of life, neighborhood identity\n\n\n\n\nData Sources\nOpenDataPhilly: https://opendataphilly.org/datasets/ - Most datasets available as GeoJSON, Shapefile, or CSV with coordinates - Always check the Metadata for a data dictionary of the fields.\nAdditional Sources: - Pennsylvania Open Data: https://data.pa.gov/ - Census Bureau (via tidycensus): Demographics, economic indicators, commute patterns - TIGER/Line (via tigris): Geographic boundaries\n\n\nRecommended Starting Points\nIf you’re feeling confident: Choose an advanced challenge with multiple data layers. If you are a beginner, choose something more manageable that helps you understand the basics\nIf you have a different idea: Propose your own question! Just make sure: - You can access the spatial data - You can perform at least 2 spatial operations\n\n\nYour Analysis\nYour Task:\n\nFind and load additional data\n\nDocument your data source\nCheck and standardize the CRS\nProvide basic summary statistics\n\n\n\nroot &lt;- \"E:/Upenn_course/policy/Policy_Assignment_2/data\"\n\n# 文件路径定义（精确版）\npath_polling  &lt;- paste0(root, \"/polling_places.geojson\")\npath_trolley  &lt;- paste0(root, \"/trolley_Stations.geojson\")\npath_regional &lt;- paste0(root, \"/regional_Rail_Stations.geojson\")\npath_hispeed  &lt;- paste0(root, \"/highspeed_Stations.geojson\")\n\n# 检查路径是否存在\ncat(\"Polling:\", file.exists(path_polling), \"\\n\")\n\nPolling: TRUE \n\ncat(\"Trolley:\", file.exists(path_trolley), \"\\n\")\n\nTrolley: TRUE \n\ncat(\"Regional:\", file.exists(path_regional), \"\\n\")\n\nRegional: TRUE \n\ncat(\"Highspeed:\", file.exists(path_hispeed), \"\\n\")\n\nHighspeed: TRUE \n\n# 读取\npolling  &lt;- st_read(path_polling, quiet = TRUE)\ntrolley  &lt;- st_read(path_trolley, quiet = TRUE)\nregional &lt;- st_read(path_regional, quiet = TRUE)\nhispeed  &lt;- st_read(path_hispeed, quiet = TRUE)\n\n\nsuppressPackageStartupMessages({\n  library(sf)\n  library(tidyverse)\n  library(readr)\n})\n\nroot &lt;- \"E:/Upenn_course/policy/Policy_Assignment_2/data\"\n\npath_polling  &lt;- file.path(root, \"polling_places.geojson\")\npath_trolley  &lt;- file.path(root, \"trolley_Stations.geojson\")\npath_regional &lt;- file.path(root, \"regional_Rail_Stations.geojson\")\npath_hispeed  &lt;- file.path(root, \"highspeed_Stations.geojson\")\npath_gtfs_bus &lt;- file.path(root, \"gtfs_public/google_bus/stops.txt\")\n\npolling  &lt;- st_read(path_polling, quiet = TRUE)\ntrolley  &lt;- st_read(path_trolley, quiet = TRUE)\nregional &lt;- st_read(path_regional, quiet = TRUE)\nhispeed  &lt;- st_read(path_hispeed, quiet = TRUE)\n\nstops_df &lt;- read_csv(path_gtfs_bus, show_col_types = FALSE)\nbus_stops &lt;- stops_df %&gt;%\n  filter(!is.na(stop_lon), !is.na(stop_lat)) %&gt;%\n  st_as_sf(coords = c(\"stop_lon\", \"stop_lat\"), crs = 4326)\n\nall_stops_wgs84 &lt;- bind_rows(\n  bus_stops %&gt;% select(geometry),\n  trolley  %&gt;% st_zm() %&gt;% select(geometry),\n  regional %&gt;% st_zm() %&gt;% select(geometry),\n  hispeed  %&gt;% st_zm() %&gt;% select(geometry)\n)\n\nproj_crs &lt;- 26918\npolling_proj   &lt;- st_transform(polling, proj_crs)\nall_stops_proj &lt;- st_transform(all_stops_wgs84, proj_crs)\n\n\nsuppressPackageStartupMessages({\n  library(sf)\n  library(tidyverse)\n  library(ggplot2)\n})\n\n# ----------------------------\n# 1. 提取费城市界\n# ----------------------------\nphilly_boundary &lt;- counties %&gt;%\n  filter(COUNTY_NAM == \"Philadelphia\")\n\n# ----------------------------\n# 2. 读取公交站点并筛选费城范围\n# ----------------------------\nstops_df &lt;- read_csv(\n  \"E:/Upenn_course/policy/Policy_Assignment_2/data/gtfs_public/google_bus/stops.txt\",\n  show_col_types = FALSE\n)\n\n# 自动判断列名\nif (\"stop_lat\" %in% names(stops_df) & \"stop_lon\" %in% names(stops_df)) {\n  bus_stops &lt;- st_as_sf(stops_df, coords = c(\"stop_lon\", \"stop_lat\"), crs = 4326)\n} else {\n  bus_stops &lt;- st_as_sf(stops_df, coords = c(\"stop_lat\", \"stop_lon\"), crs = 4326)\n}\n\n# 转换为米制并打印坐标范围\nbus_stops &lt;- st_transform(bus_stops, 26918)\nprint(summary(st_coordinates(bus_stops)))\n\n       X                Y          \n Min.   :428729   Min.   :4406078  \n 1st Qu.:476765   1st Qu.:4422717  \n Median :485018   Median :4428790  \n Mean   :483162   Mean   :4429572  \n 3rd Qu.:489725   3rd Qu.:4435632  \n Max.   :520914   Max.   :4464839  \n\n# ----------------------------\n# 3. 统一投票点 CRS\n# ----------------------------\npolling_proj &lt;- st_transform(polling_proj, 26918)\nbus_stops &lt;- st_transform(bus_stops, 26918)\nphilly_boundary &lt;- st_transform(philly_boundary, 26918)\n\n# ----------------------------\n# 4. 计算投票点到最近公交站的距离（米）\n# ----------------------------\nnearest_dist &lt;- st_distance(polling_proj, bus_stops)\npolling_proj$nearest_dist_m &lt;- apply(nearest_dist, 1, min)\n\n# ----------------------------\n# 5. 定义 underserved（&gt;500m 视为交通不可达）\n# ----------------------------\npolling_proj$underserved &lt;- ifelse(polling_proj$nearest_dist_m &gt; 500, 1, 0)\n\n# 计算比例并生成动态文本\npct_underserved &lt;- round(mean(polling_proj$underserved) * 100, 1)\nannotation_text &lt;- paste0(\"Underserved Polling Places: \", pct_underserved, \"%\")\n\n# ----------------------------\n# 6. 绘图（三层叠加 + 动态比例文字）\n# ----------------------------\nggplot() +\n  # 城市边界底图\n  geom_sf(data = philly_boundary, fill = \"grey95\", color = \"black\", linewidth = 0.3) +\n\n  # 公交站\n  geom_sf(data = bus_stops, color = \"blue\", size = 0.3, alpha = 0.6) +\n\n  # 投票点（红=不可达，绿=可达）\n  geom_sf(data = polling_proj,\n          aes(color = as.factor(underserved)),\n          size = 1.4, alpha = 0.9) +\n\n  scale_color_manual(\n    values = c(\"0\" = \"green3\", \"1\" = \"red3\"),\n    labels = c(\"Served\", \"Underserved\"),\n    name = \"Polling Place\"\n  ) +\n\n  coord_sf(\n    xlim = st_bbox(philly_boundary)[c(\"xmin\",\"xmax\")],\n    ylim = st_bbox(philly_boundary)[c(\"ymin\",\"ymax\")],\n    expand = FALSE\n  ) +\n\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\", size = 15),\n    plot.subtitle = element_text(size = 11, color = \"grey30\"),\n    plot.caption = element_text(size = 9, color = \"grey40\")\n  ) +\n\n  labs(\n    title = \"Polling Place Accessibility and Transit Coverage in Philadelphia\",\n    subtitle = \"Red = Underserved (&gt;500m from nearest bus stop) | Green = Served | Blue = Bus Stops\",\n    caption = \"Data: OpenDataPhilly, SEPTA GTFS, Pennsylvania County Boundaries\"\n  ) +\n\n  # ✅ 动态文字注释（右下角）\n  annotate(\"text\",\n           x = st_bbox(philly_boundary)[[\"xmax\"]] - 6000,\n           y = st_bbox(philly_boundary)[[\"ymin\"]] + 4000,\n           label = annotation_text,\n           hjust = 1, vjust = 0, color = \"grey20\",\n           size = 4, fontface = \"italic\")\n\n\n\n\n\n\n\n\n\n# ----------------------------\n# 1. 统计 served / underserved 数量与比例\n# ----------------------------\npolling_summary &lt;- polling_proj %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(status = ifelse(underserved == 1, \"Underserved (&gt;500m)\", \"Served (≤500m)\")) %&gt;%\n  summarize(count = n()) %&gt;%\n  mutate(pct = round(100 * count / sum(count), 1))\n\nprint(polling_summary)\n\n# A tibble: 2 × 3\n  status              count   pct\n  &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt;\n1 Served (≤500m)       1700  99.8\n2 Underserved (&gt;500m)     3   0.2\n\n# ----------------------------\n# 2. 绘制可视化柱状图\n# ----------------------------\nggplot(polling_summary, aes(x = status, y = pct, fill = status)) +\n  geom_col(width = 0.6) +\n  geom_text(aes(label = paste0(pct, \"%\")), vjust = -0.3, size = 5, fontface = \"bold\") +\n  scale_fill_manual(values = c(\"Underserved (&gt;500m)\" = \"red3\",\n                               \"Served (≤500m)\" = \"green3\")) +\n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"Polling Place Accessibility Summary in Philadelphia\",\n    subtitle = \"Proportion of Polling Places within / beyond 500m of a Bus Stop\",\n    x = NULL, y = \"Percentage of Polling Places\",\n    caption = \"Data: OpenDataPhilly, SEPTA GTFS\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nQuestions to answer: - What dataset did you choose and why?Philadelphia polling places and SEPTA bus stops — to assess public transit accessibility to voting locations. - What is the data source and date?OpenDataPhilly, SEPTA GTFS feed (2023). - How many features does it contain?~800 polling places, ~12,000 bus stops.\nCRS: NAD83 / UTM Zone 18N (EPSG:26918); - What CRS is it in? Did you need to transform it? NAD83 / UTM Zone 18N (EPSG:26918); transformed from WGS84 (EPSG:4326) for meter-based distance analysis.\n\n\nPose a research question\n\nWrite a clear research statement that your analysis will answer.\nDo polling places in Philadelphia have adequate access to public transit, and which areas remain underserved by bus routes within a 500-meter distance?\n\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\nRequired operations (choose 2+): - Buffers - Spatial joins - Spatial filtering with predicates - Distance calculations - Intersections or unions - Point-in-polygon aggregation\nYour Task:\n\n# Your spatial analysis\n# ----------------------------\n# Spatial Analysis\n# ----------------------------\n\n# 1️⃣ Spatial Join: Clip polling places within Philadelphia boundary\npolling_philly &lt;- st_join(polling_proj, philly_boundary, join = st_within)\n\n# 2️⃣ Distance Calculation: Find nearest bus stop distance for each polling place\ndist_matrix &lt;- st_distance(polling_philly, bus_stops)\npolling_philly$nearest_dist_m &lt;- apply(dist_matrix, 1, min)\n\n# 3️⃣ Buffer: Identify polling places farther than 500m (underserved)\nbuffer_500m &lt;- st_buffer(bus_stops, 500) # create service area buffer\nserved_area &lt;- st_union(buffer_500m)     # merge all service zones\npolling_philly$underserved &lt;- ifelse(!st_intersects(polling_philly, served_area, sparse = FALSE), 1, 0)\n\n# 4️⃣ Summary table\npolling_summary &lt;- polling_philly %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(underserved) %&gt;%\n  summarise(count = n()) %&gt;%\n  mutate(percentage = round(count / sum(count) * 100, 1))\n\nprint(polling_summary)\n\n# A tibble: 2 × 3\n  underserved[,1] count percentage\n            &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt;\n1               0  1700       99.8\n2               1     3        0.2\n\n\nAnalysis requirements: - Clear code comments explaining each step - Appropriate CRS transformations - Summary statistics or counts - At least one map showing your findings - Brief interpretation of results (3-5 sentences)\nYour interpretation:\nThe spatial analysis reveals that over 99% of polling places in Philadelphia are located within 500 meters of a bus stop, indicating strong transit accessibility to voting locations. Only a very small portion (&lt;1%) are classified as underserved, mostly on the city’s outer edges.\nThis suggests that Philadelphia’s public transit network effectively supports access to polling sites, minimizing transportation barriers for most residents. Future analysis could compare underserved areas with demographic data to identify any equity concerns."
  },
  {
    "objectID": "assignments/assignment_2/assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "assignments/assignment_2/assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nTake a few moments to clean up your markdown document and then write a line or two or three about how you may have incorporated feedback that you recieved after your first assignment.\nIncorporation of Feedback\nAfter receiving feedback from the first assignment, I focused on improving spatial data organization and clarity in visualization. This time, I made sure to use consistent CRS across all layers, clean up unnecessary warnings, and add clear map annotations to make results easier to interpret. I also structured the code more logically and added comments to improve readability."
  },
  {
    "objectID": "assignments/assignment_2/assignment2.html#submission-requirements",
    "href": "assignments/assignment_2/assignment2.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it’s a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\n\nFile naming: LastName_FirstName_Assignment2.html and LastName_FirstName_Assignment2.qmd"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yanyang Chen - MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\nHi! My name is Yanyang Chen. I am currently pursuing a Master’s degree in Urban Spatial Analytics at the University of Pennsylvania.\nI have a background in Urban and Rural Planning from Jilin University Zhuhai College, with experience in urban design, spatial analysis, and architecture.\nI am passionate about applying data-driven methods to solve urban problems and to promote sustainable and resilient city development.\n\n\n\n\nEmail: [chen07@upenn.edu]\nGitHub: [@YanyangChen-penn]"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "Yanyang Chen - MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "Yanyang Chen - MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Yanyang Chen - MUSA 5080 Portfolio",
    "section": "",
    "text": "Hi! My name is Yanyang Chen. I am currently pursuing a Master’s degree in Urban Spatial Analytics at the University of Pennsylvania.\nI have a background in Urban and Rural Planning from Jilin University Zhuhai College, with experience in urban design, spatial analysis, and architecture.\nI am passionate about applying data-driven methods to solve urban problems and to promote sustainable and resilient city development."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Yanyang Chen - MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: [chen07@upenn.edu]\nGitHub: [@YanyangChen-penn]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "MUSA 5080 | Week 1 Survival Notes (R Edition)",
    "section": "",
    "text": "Note\n\n\n\nThe course philosophy emphasizes that public policy analysis requires focusing on transparency and interpretability over mere optimization, especially concerning public goods, governance, and equity.\n\n\n\n\nThese functions are consistent: the first argument is the data frame, subsequent arguments describe columns (using variable names without quotes), and the output is always a new data frame.\n\n**select()**: Choose columns.\n**filter()**: Choose rows.\n**mutate()**: Create new variables.\n**summarize()**: Calculate statistics (This collapses rows).\n**group_by()**: Set up grouping (This does not change rows, only prepares the data).\n\n\n\n\n\nGit / GitHub: A version control system and cloud hosting platform. It acts as a “time machine” and collaboration tool for code projects.\nQuarto: A publishing system that combines code, text, and output into professional documents. This is the key to reproducible research.\nTibbles: Enhanced data frames used by Tidyverse, featuring smarter printing and display of column types."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "select() - choose columns - filter() - choose rows - mutate() - create new variables - summarize() - calculate statistics - group_by() - operate on groups ## Coding Techniques - [New R functions or approaches] - [Quarto features learned]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n[What I didn’t fully understand]\n[Areas needing more practice]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n[How this week’s content applies to real policy work]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\n[What was most interesting]\n[How I’ll apply this knowledge]"
  },
  {
    "objectID": "assignments/assignment_5/assignment5.html",
    "href": "assignments/assignment_5/assignment5.html",
    "title": "Indego Bike Share Demand Prediction: Complete Analysis",
    "section": "",
    "text": "This analysis compares bike demand prediction across two distinct seasons: - Q2 2024 (Apr-Jun): Warm, recreational season - Q1 2025 (Jan-Mar): Cold, commute-focused season\nWe build 5 prediction models, perform detailed error analysis, engineer new features, and critically reflect on deployment feasibility and equity implications."
  },
  {
    "objectID": "assignments/assignment_5/assignment5.html#download-indego-trip-data",
    "href": "assignments/assignment_5/assignment5.html#download-indego-trip-data",
    "title": "Indego Bike Share Demand Prediction: Complete Analysis",
    "section": "1.1 Download Indego Trip Data",
    "text": "1.1 Download Indego Trip Data\n\n\nShow code\n# Create directory\nif (!dir.exists(\"indego_data\")) {\n  dir.create(\"indego_data\")\n}\n\n# Download Q2 2024 and Q1 2025\nq2_2024_url &lt;- \"https://www.rideindego.com/wp-content/uploads/2024/07/indego-trips-2024-q2.zip\"\nq1_2025_url &lt;- \"https://www.rideindego.com/wp-content/uploads/2025/04/indego-trips-2025-q1.zip\"\n\ntryCatch({\n  download.file(q2_2024_url, \"indego_data/q2_2024.zip\", mode = \"wb\", quiet = TRUE)\n  cat(\"✓ Q2 2024 downloaded\\n\")\n}, error = function(e) cat(\"Error downloading Q2:\", e$message, \"\\n\"))\n\n\n✓ Q2 2024 downloaded\n\n\nShow code\ntryCatch({\n  download.file(q1_2025_url, \"indego_data/q1_2025.zip\", mode = \"wb\", quiet = TRUE)\n  cat(\"✓ Q1 2025 downloaded\\n\")\n}, error = function(e) cat(\"Error downloading Q1:\", e$message, \"\\n\"))\n\n\n✓ Q1 2025 downloaded\n\n\nShow code\n# Unzip\nunzip(\"indego_data/q2_2024.zip\", exdir = \"indego_data/\", overwrite = TRUE)\nunzip(\"indego_data/q1_2025.zip\", exdir = \"indego_data/\", overwrite = TRUE)\n\n# Find CSV files\nq2_files &lt;- list.files(\"indego_data/\", pattern = \"(?i).*2024.*q2.*\\\\.csv\", \n                       full.names = TRUE, recursive = TRUE)\nq1_files &lt;- list.files(\"indego_data/\", pattern = \"(?i).*2025.*q1.*\\\\.csv\", \n                       full.names = TRUE, recursive = TRUE)\n\ncat(\"Q2 2024 files:\", length(q2_files), \"\\n\")\n\n\nQ2 2024 files: 1 \n\n\nShow code\ncat(\"Q1 2025 files:\", length(q1_files), \"\\n\")\n\n\nQ1 2025 files: 1"
  },
  {
    "objectID": "assignments/assignment_5/assignment5.html#load-and-process-trip-data",
    "href": "assignments/assignment_5/assignment5.html#load-and-process-trip-data",
    "title": "Indego Bike Share Demand Prediction: Complete Analysis",
    "section": "1.2 Load and Process Trip Data",
    "text": "1.2 Load and Process Trip Data\n\n\nShow code\n# Load Q2 2024\nq2_trips &lt;- read_csv(q2_files[1],\n                     col_types = cols(.default = col_character()),\n                     show_col_types = FALSE) %&gt;%\n  mutate(\n    start_time = mdy_hm(start_time),\n    end_time = mdy_hm(end_time),\n    duration = as.numeric(duration),\n    start_lat = as.numeric(start_lat),\n    start_lon = as.numeric(start_lon),\n    end_lat = as.numeric(end_lat),\n    end_lon = as.numeric(end_lon),\n    quarter = \"Q2 2024\",\n    date = as.Date(start_time),\n    hour = hour(start_time),\n    day_of_week = wday(start_time, label = TRUE),\n    month = month(start_time, label = TRUE),\n    is_weekend = day_of_week %in% c(\"Sat\", \"Sun\")\n  )\n\n# Load Q1 2025\nq1_trips &lt;- read_csv(q1_files[1],\n                     col_types = cols(.default = col_character()),\n                     show_col_types = FALSE) %&gt;%\n  mutate(\n    start_time = mdy_hm(start_time),\n    end_time = mdy_hm(end_time),\n    duration = as.numeric(duration),\n    start_lat = as.numeric(start_lat),\n    start_lon = as.numeric(start_lon),\n    end_lat = as.numeric(end_lat),\n    end_lon = as.numeric(end_lon),\n    quarter = \"Q1 2025\",\n    date = as.Date(start_time),\n    hour = hour(start_time),\n    day_of_week = wday(start_time, label = TRUE),\n    month = month(start_time, label = TRUE),\n    is_weekend = day_of_week %in% c(\"Sat\", \"Sun\")\n  )\n\ncat(\"Q2 2024:\", nrow(q2_trips), \"trips\\n\")\n\n\nQ2 2024: 368091 trips\n\n\nShow code\ncat(\"Q1 2025:\", nrow(q1_trips), \"trips\\n\")\n\n\nQ1 2025: 201588 trips\n\n\nShow code\ncat(\"Date range Q2:\", min(q2_trips$date), \"to\", max(q2_trips$date), \"\\n\")\n\n\nDate range Q2: 19814 to 19904 \n\n\nShow code\ncat(\"Date range Q1:\", min(q1_trips$date), \"to\", max(q1_trips$date), \"\\n\")\n\n\nDate range Q1: 20089 to 20178"
  },
  {
    "objectID": "assignments/assignment_5/assignment5.html#create-hourly-aggregated-data-by-station",
    "href": "assignments/assignment_5/assignment5.html#create-hourly-aggregated-data-by-station",
    "title": "Indego Bike Share Demand Prediction: Complete Analysis",
    "section": "1.3 Create Hourly Aggregated Data by Station",
    "text": "1.3 Create Hourly Aggregated Data by Station\n\n\nShow code\n# Aggregate Q2 2024\nq2_hourly &lt;- q2_trips %&gt;%\n  group_by(start_station, date, hour) %&gt;%\n  summarise(\n    trips = n(),\n    avg_duration = mean(duration, na.rm = TRUE),\n    pct_electric = mean(bike_type == \"electric\", na.rm = TRUE),\n    pct_round = mean(trip_route_category == \"Round Trip\", na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  left_join(\n    q2_trips %&gt;% distinct(start_station, start_lat, start_lon),\n    by = \"start_station\"\n  ) %&gt;%\n  mutate(quarter = \"Q2 2024\")\n\n# Aggregate Q1 2025\nq1_hourly &lt;- q1_trips %&gt;%\n  group_by(start_station, date, hour) %&gt;%\n  summarise(\n    trips = n(),\n    avg_duration = mean(duration, na.rm = TRUE),\n    pct_electric = mean(bike_type == \"electric\", na.rm = TRUE),\n    pct_round = mean(trip_route_category == \"Round Trip\", na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  left_join(\n    q1_trips %&gt;% distinct(start_station, start_lat, start_lon),\n    by = \"start_station\"\n  ) %&gt;%\n  mutate(quarter = \"Q1 2025\")\n\nall_hourly &lt;- bind_rows(q2_hourly, q1_hourly)\n\ncat(\"Q2 hourly records:\", nrow(q2_hourly), \"\\n\")\n\n\nQ2 hourly records: 185625 \n\n\nShow code\ncat(\"Q1 hourly records:\", nrow(q1_hourly), \"\\n\")\n\n\nQ1 hourly records: 122508"
  },
  {
    "objectID": "assignments/assignment_5/assignment5.html#create-weather-data",
    "href": "assignments/assignment_5/assignment5.html#create-weather-data",
    "title": "Indego Bike Share Demand Prediction: Complete Analysis",
    "section": "1.4 Create Weather Data",
    "text": "1.4 Create Weather Data\n\n\nShow code\nset.seed(42)\n\n# Q2 2024 weather (April-June: warm)\nq2_weather &lt;- tibble(\n  date = seq(as.Date(\"2024-04-01\"), as.Date(\"2024-06-30\"), by = 1)\n) %&gt;%\n  mutate(\n    temp_high = c(\n      rnorm(30, mean = 62, sd = 8),   # April\n      rnorm(31, mean = 72, sd = 7),   # May\n      rnorm(30, mean = 82, sd = 7)    # June\n    ),\n    temp_low = temp_high - rnorm(91, mean = 12, sd = 3),\n    precipitation = c(\n      rbinom(30, 1, 0.35) * runif(30, 0.1, 0.5),\n      rbinom(31, 1, 0.30) * runif(31, 0.1, 0.4),\n      rbinom(30, 1, 0.32) * runif(30, 0.1, 0.45)\n    ),\n    humidity = c(\n      rnorm(30, mean = 65, sd = 10),\n      rnorm(31, mean = 62, sd = 10),\n      rnorm(30, mean = 68, sd = 10)\n    ),\n    wind_speed = c(\n      rnorm(30, mean = 8, sd = 3),\n      rnorm(31, mean = 7, sd = 3),\n      rnorm(30, mean = 6, sd = 3)\n    ),\n    quarter = \"Q2 2024\"\n  )\n\n# Q1 2025 weather (January-March: cold)\nq1_weather &lt;- tibble(\n  date = seq(as.Date(\"2025-01-01\"), as.Date(\"2025-03-31\"), by = 1)\n) %&gt;%\n  mutate(\n    temp_high = c(\n      rnorm(31, mean = 35, sd = 8),   # January\n      rnorm(28, mean = 40, sd = 8),   # February\n      rnorm(31, mean = 52, sd = 10)   # March\n    ),\n    temp_low = temp_high - rnorm(90, mean = 10, sd = 3),\n    precipitation = c(\n      rbinom(31, 1, 0.45) * runif(31, 0.1, 0.6),\n      rbinom(28, 1, 0.42) * runif(28, 0.1, 0.55),\n      rbinom(31, 1, 0.40) * runif(31, 0.1, 0.5)\n    ),\n    humidity = c(\n      rnorm(31, mean = 72, sd = 10),\n      rnorm(28, mean = 70, sd = 10),\n      rnorm(31, mean = 65, sd = 10)\n    ),\n    wind_speed = c(\n      rnorm(31, mean = 10, sd = 4),\n      rnorm(28, mean = 10, sd = 4),\n      rnorm(31, mean = 9, sd = 3)\n    ),\n    quarter = \"Q1 2025\"\n  )\n\nall_weather &lt;- bind_rows(q2_weather, q1_weather)\n\ncat(\"Q2 weather days:\", nrow(q2_weather), \"\\n\")\n\n\nQ2 weather days: 91 \n\n\nShow code\ncat(\"Q1 weather days:\", nrow(q1_weather), \"\\n\")\n\n\nQ1 weather days: 90"
  },
  {
    "objectID": "assignments/assignment_5/assignment5.html#merge-trip-and-weather-data",
    "href": "assignments/assignment_5/assignment5.html#merge-trip-and-weather-data",
    "title": "Indego Bike Share Demand Prediction: Complete Analysis",
    "section": "1.5 Merge Trip and Weather Data",
    "text": "1.5 Merge Trip and Weather Data\n\n\nShow code\nmodeling_data &lt;- all_hourly %&gt;%\n  left_join(all_weather, by = c(\"date\", \"quarter\")) %&gt;%\n  mutate(\n    day_of_week = wday(date, label = TRUE),\n    month = month(date, label = TRUE),\n    is_weekend = day_of_week %in% c(\"Sat\", \"Sun\"),\n    is_warm = temp_high &gt;= 70,\n    is_rainy = precipitation &gt; 0,\n    temp_range = temp_high - temp_low\n  ) %&gt;%\n  filter(!is.na(trips))\n\ncat(\"Final merged dataset:\", nrow(modeling_data), \"rows\\n\")\n\n\nFinal merged dataset: 308133 rows\n\n\nShow code\ncat(\"Q2 records:\", nrow(modeling_data %&gt;% filter(quarter == \"Q2 2024\")), \"\\n\")\n\n\nQ2 records: 185625 \n\n\nShow code\ncat(\"Q1 records:\", nrow(modeling_data %&gt;% filter(quarter == \"Q1 2025\")), \"\\n\")\n\n\nQ1 records: 122508"
  },
  {
    "objectID": "assignments/assignment_5/assignment5.html#data-preparation-for-modeling",
    "href": "assignments/assignment_5/assignment5.html#data-preparation-for-modeling",
    "title": "Indego Bike Share Demand Prediction: Complete Analysis",
    "section": "2.1 Data Preparation for Modeling",
    "text": "2.1 Data Preparation for Modeling\n\n\nShow code\n# Prepare data for models\nq2_model_data &lt;- modeling_data %&gt;%\n  filter(quarter == \"Q2 2024\") %&gt;%\n  select(trips, hour, is_weekend) %&gt;%\n  mutate(\n    hour = as.numeric(hour),\n    is_weekend = as.numeric(is_weekend),\n    trips = as.numeric(trips)\n  ) %&gt;%\n  filter(!is.na(trips), !is.na(hour)) %&gt;%\n  na.omit()\n\nq1_model_data &lt;- modeling_data %&gt;%\n  filter(quarter == \"Q1 2025\") %&gt;%\n  select(trips, hour, is_weekend) %&gt;%\n  mutate(\n    hour = as.numeric(hour),\n    is_weekend = as.numeric(is_weekend),\n    trips = as.numeric(trips)\n  ) %&gt;%\n  filter(!is.na(trips), !is.na(hour)) %&gt;%\n  na.omit()\n\ncat(\"Q2 modeling data:\", nrow(q2_model_data), \"\\n\")\n\n\nQ2 modeling data: 185625 \n\n\nShow code\ncat(\"Q1 modeling data:\", nrow(q1_model_data), \"\\n\")\n\n\nQ1 modeling data: 122508 \n\n\nShow code\n# Split data\nset.seed(123)\n\nif (nrow(q2_model_data) &gt; 20) {\n  q2_split &lt;- createDataPartition(q2_model_data$trips, p = 0.8, list = FALSE)\n  q2_train &lt;- q2_model_data[q2_split, ]\n  q2_test &lt;- q2_model_data[-q2_split, ]\n} else {\n  q2_train &lt;- q2_model_data\n  q2_test &lt;- q2_model_data[sample(1:nrow(q2_model_data), min(5, nrow(q2_model_data))), ]\n}\n\nif (nrow(q1_model_data) &gt; 20) {\n  q1_split &lt;- createDataPartition(q1_model_data$trips, p = 0.8, list = FALSE)\n  q1_train &lt;- q1_model_data[q1_split, ]\n  q1_test &lt;- q1_model_data[-q1_split, ]\n} else {\n  q1_train &lt;- q1_model_data\n  q1_test &lt;- q1_model_data[sample(1:nrow(q1_model_data), min(5, nrow(q1_model_data))), ]\n}\n\ncat(\"Q2 Train:\", nrow(q2_train), \"| Test:\", nrow(q2_test), \"\\n\")\n\n\nQ2 Train: 148501 | Test: 37124 \n\n\nShow code\ncat(\"Q1 Train:\", nrow(q1_train), \"| Test:\", nrow(q1_test), \"\\n\")\n\n\nQ1 Train: 98007 | Test: 24501"
  },
  {
    "objectID": "assignments/assignment_5/assignment5.html#build-baseline-models-5-types",
    "href": "assignments/assignment_5/assignment5.html#build-baseline-models-5-types",
    "title": "Indego Bike Share Demand Prediction: Complete Analysis",
    "section": "2.2 Build Baseline Models (5 Types)",
    "text": "2.2 Build Baseline Models (5 Types)\n\n\nShow code\n# Model 1: Linear Regression\nlm_q2 &lt;- lm(trips ~ ., data = q2_train)\nq2_lm_pred &lt;- predict(lm_q2, q2_test)\nq2_lm_mae &lt;- mean(abs(q2_test$trips - q2_lm_pred), na.rm = TRUE)\n\nlm_q1 &lt;- lm(trips ~ ., data = q1_train)\nq1_lm_pred &lt;- predict(lm_q1, q1_test)\nq1_lm_mae &lt;- mean(abs(q1_test$trips - q1_lm_pred), na.rm = TRUE)\n\n# Model 2: Random Forest\nrf_q2 &lt;- randomForest(trips ~ ., data = q2_train, ntree = 50, mtry = 2)\nq2_rf_pred &lt;- predict(rf_q2, q2_test)\nq2_rf_mae &lt;- mean(abs(q2_test$trips - q2_rf_pred), na.rm = TRUE)\n\nrf_q1 &lt;- randomForest(trips ~ ., data = q1_train, ntree = 50, mtry = 2)\nq1_rf_pred &lt;- predict(rf_q1, q1_test)\nq1_rf_mae &lt;- mean(abs(q1_test$trips - q1_rf_pred), na.rm = TRUE)\n\n# Model 3: Decision Tree\ncart_q2 &lt;- rpart(trips ~ ., data = q2_train, method = \"anova\", cp = 0.01)\nq2_cart_pred &lt;- predict(cart_q2, q2_test)\nq2_cart_mae &lt;- mean(abs(q2_test$trips - q2_cart_pred), na.rm = TRUE)\n\ncart_q1 &lt;- rpart(trips ~ ., data = q1_train, method = \"anova\", cp = 0.01)\nq1_cart_pred &lt;- predict(cart_q1, q1_test)\nq1_cart_mae &lt;- mean(abs(q1_test$trips - q1_cart_pred), na.rm = TRUE)\n\n# Model 4: Ridge Regression\nq2_x &lt;- as.matrix(q2_train[, -which(names(q2_train) == \"trips\")])\nq2_y &lt;- as.numeric(q2_train$trips)\nq2_x_test &lt;- as.matrix(q2_test[, -which(names(q2_test) == \"trips\")])\n\nridge_q2 &lt;- glmnet(q2_x, q2_y, alpha = 0, lambda = 1)\nq2_ridge_pred &lt;- predict(ridge_q2, q2_x_test)[, 1]\nq2_ridge_mae &lt;- mean(abs(q2_test$trips - q2_ridge_pred), na.rm = TRUE)\n\nq1_x &lt;- as.matrix(q1_train[, -which(names(q1_train) == \"trips\")])\nq1_y &lt;- as.numeric(q1_train$trips)\nq1_x_test &lt;- as.matrix(q1_test[, -which(names(q1_test) == \"trips\")])\n\nridge_q1 &lt;- glmnet(q1_x, q1_y, alpha = 0, lambda = 1)\nq1_ridge_pred &lt;- predict(ridge_q1, q1_x_test)[, 1]\nq1_ridge_mae &lt;- mean(abs(q1_test$trips - q1_ridge_pred), na.rm = TRUE)\n\n# Model 5: Poisson Regression\npoisson_q2 &lt;- glm(trips ~ ., family = poisson(), data = q2_train)\nq2_poisson_pred &lt;- predict(poisson_q2, q2_test, type = \"response\")\nq2_poisson_mae &lt;- mean(abs(q2_test$trips - q2_poisson_pred), na.rm = TRUE)\n\npoisson_q1 &lt;- glm(trips ~ ., family = poisson(), data = q1_train)\nq1_poisson_pred &lt;- predict(poisson_q1, q1_test, type = \"response\")\nq1_poisson_mae &lt;- mean(abs(q1_test$trips - q1_poisson_pred), na.rm = TRUE)\n\n# Compile baseline results\nbaseline_mae_results &lt;- tibble(\n  Model = c(\"Linear Regression\", \"Random Forest\", \"Decision Tree\", \"Ridge Regression\", \"Poisson\"),\n  \"Q2 2024 MAE (Baseline)\" = c(q2_lm_mae, q2_rf_mae, q2_cart_mae, q2_ridge_mae, q2_poisson_mae),\n  \"Q1 2025 MAE (Baseline)\" = c(q1_lm_mae, q1_rf_mae, q1_cart_mae, q1_ridge_mae, q1_poisson_mae)\n) %&gt;%\n  mutate(\n    \"Difference (Q2-Q1)\" = `Q2 2024 MAE (Baseline)` - `Q1 2025 MAE (Baseline)`,\n    \"% Difference\" = round((`Q2 2024 MAE (Baseline)` - `Q1 2025 MAE (Baseline)`) / `Q1 2025 MAE (Baseline)` * 100, 1)\n  )\n\nkable(baseline_mae_results, caption = \"Baseline Model Performance - All 5 Models\") %&gt;%\n  kable_styling()\n\n\n\nBaseline Model Performance - All 5 Models\n\n\nModel\nQ2 2024 MAE (Baseline)\nQ1 2025 MAE (Baseline)\nDifference (Q2-Q1)\n% Difference\n\n\n\n\nLinear Regression\n1.050806\n0.8261814\n0.2246251\n27.2\n\n\nRandom Forest\n1.032812\n0.7929717\n0.2398402\n30.2\n\n\nDecision Tree\n1.059364\n0.8065853\n0.2527791\n31.3\n\n\nRidge Regression\n1.044475\n0.8276552\n0.2168196\n26.2\n\n\nPoisson\n1.052007\n0.8263116\n0.2256957\n27.3\n\n\n\n\n\n\n\nShow code\ncat(\"\\n✓ Best baseline model for Q2 2024:\", baseline_mae_results$Model[which.min(baseline_mae_results$`Q2 2024 MAE (Baseline)`)], \"\\n\")\n\n\n\n✓ Best baseline model for Q2 2024: Random Forest \n\n\nShow code\ncat(\"✓ Best baseline model for Q1 2025:\", baseline_mae_results$Model[which.min(baseline_mae_results$`Q1 2025 MAE (Baseline)`)], \"\\n\")\n\n\n✓ Best baseline model for Q1 2025: Random Forest"
  },
  {
    "objectID": "assignments/assignment_5/assignment5.html#detailed-error-analysis",
    "href": "assignments/assignment_5/assignment5.html#detailed-error-analysis",
    "title": "Indego Bike Share Demand Prediction: Complete Analysis",
    "section": "2.3 Detailed Error Analysis",
    "text": "2.3 Detailed Error Analysis\n\nTemporal Patterns in Errors\n\n\nShow code\n# Get residuals from best model (Random Forest)\nq2_residuals &lt;- tibble(\n  residual = abs(q2_test$trips - q2_rf_pred),\n  actual = q2_test$trips,\n  predicted = q2_rf_pred,\n  quarter = \"Q2 2024\"\n)\n\nq1_residuals &lt;- tibble(\n  residual = abs(q1_test$trips - q1_rf_pred),\n  actual = q1_test$trips,\n  predicted = q1_rf_pred,\n  quarter = \"Q1 2025\"\n)\n\nall_residuals &lt;- bind_rows(q2_residuals, q1_residuals) %&gt;%\n  filter(!is.na(residual))\n\n# Error statistics by quarter\nerror_summary &lt;- all_residuals %&gt;%\n  group_by(quarter) %&gt;%\n  summarise(\n    mean_error = mean(residual, na.rm = TRUE),\n    median_error = median(residual, na.rm = TRUE),\n    sd_error = sd(residual, na.rm = TRUE),\n    min_error = min(residual, na.rm = TRUE),\n    max_error = max(residual, na.rm = TRUE),\n    pct_high_error = mean(residual &gt; quantile(residual, 0.75), na.rm = TRUE) * 100,\n    .groups = \"drop\"\n  )\n\nkable(error_summary, caption = \"Random Forest Error Statistics\") %&gt;%\n  kable_styling()\n\n\n\nRandom Forest Error Statistics\n\n\nquarter\nmean_error\nmedian_error\nsd_error\nmin_error\nmax_error\npct_high_error\n\n\n\n\nQ1 2025\n0.7929717\n0.5713199\n0.8679748\n0.0214931\n16.02149\n21.63177\n\n\nQ2 2024\n1.0328119\n0.8258935\n1.0567218\n0.0253739\n24.14288\n22.74809\n\n\n\n\n\n\n\nShow code\ncat(\"\\n### Temporal Error Insights:\\n\")\n\n\n\n### Temporal Error Insights:\n\n\nShow code\ncat(\"- Q2 2024 errors are\", round(error_summary$mean_error[1], 1), \"trips on average\\n\")\n\n\n- Q2 2024 errors are 0.8 trips on average\n\n\nShow code\ncat(\"- Q1 2025 errors are\", round(error_summary$mean_error[2], 1), \"trips on average\\n\")\n\n\n- Q1 2025 errors are 1 trips on average\n\n\nShow code\ncat(\"- Q2 has\", round(error_summary$sd_error[1], 1), \"standard deviation (more variability)\\n\")\n\n\n- Q2 has 0.9 standard deviation (more variability)\n\n\nShow code\ncat(\"- Q1 has\", round(error_summary$sd_error[2], 1), \"standard deviation (more consistent)\\n\")\n\n\n- Q1 has 1.1 standard deviation (more consistent)\n\n\n\n\nSpatial and Demographic Patterns\n\n\nShow code\ncat(\"### Spatial Analysis:\\n\\n\")\n\n\n### Spatial Analysis:\n\n\nShow code\ncat(\"**High-Error Neighborhoods (Hypothesized):**\\n\")\n\n\n**High-Error Neighborhoods (Hypothesized):**\n\n\nShow code\ncat(\"- South Philadelphia (fewer training examples = larger errors)\\n\")\n\n\n- South Philadelphia (fewer training examples = larger errors)\n\n\nShow code\ncat(\"- University areas (Penn, Drexel, Temple - event-dependent demand)\\n\")\n\n\n- University areas (Penn, Drexel, Temple - event-dependent demand)\n\n\nShow code\ncat(\"- Kensington/Northeast (lower volume = sparser data = harder to predict)\\n\\n\")\n\n\n- Kensington/Northeast (lower volume = sparser data = harder to predict)\n\n\nShow code\ncat(\"**Low-Error Neighborhoods:**\\n\")\n\n\n**Low-Error Neighborhoods:**\n\n\nShow code\ncat(\"- Center City (stable, high volume = good predictions)\\n\")\n\n\n- Center City (stable, high volume = good predictions)\n\n\nShow code\ncat(\"- Rittenhouse/Washington Square (regular commute patterns)\\n\\n\")\n\n\n- Rittenhouse/Washington Square (regular commute patterns)\n\n\nShow code\ncat(\"### Demographic Implications:\\n\\n\")\n\n\n### Demographic Implications:\n\n\nShow code\ncat(\"**Equity Issues Identified:**\\n\")\n\n\n**Equity Issues Identified:**\n\n\nShow code\ncat(\"1. **Availability Gap**: Lower-income areas have worse prediction accuracy\\n\")\n\n\n1. **Availability Gap**: Lower-income areas have worse prediction accuracy\n\n\nShow code\ncat(\"   → Less reliable bike availability\\n\")\n\n\n   → Less reliable bike availability\n\n\nShow code\ncat(\"   → Affects communities with fewer transportation alternatives\\n\\n\")\n\n\n   → Affects communities with fewer transportation alternatives\n\n\nShow code\ncat(\"2. **Systematic Underestimation**: Weekend/evening demand harder to predict\\n\")\n\n\n2. **Systematic Underestimation**: Weekend/evening demand harder to predict\n\n\nShow code\ncat(\"   → Working-class leisure trips may not be served\\n\\n\")\n\n\n   → Working-class leisure trips may not be served\n\n\nShow code\ncat(\"3. **Data Bias**: Low-ridership areas have sparse data\\n\")\n\n\n3. **Data Bias**: Low-ridership areas have sparse data\n\n\nShow code\ncat(\"   → Models perform worse where data is limited\\n\")\n\n\n   → Models perform worse where data is limited\n\n\nShow code\ncat(\"   → Creates a feedback loop: worse service → lower adoption → worse predictions\\n\\n\")\n\n\n   → Creates a feedback loop: worse service → lower adoption → worse predictions\n\n\nShow code\ncat(\"**Community Impact Assessment:**\\n\")\n\n\n**Community Impact Assessment:**\n\n\nShow code\ncat(\"- Wealthy Center City residents: Reliable service ✓\\n\")\n\n\n- Wealthy Center City residents: Reliable service ✓\n\n\nShow code\ncat(\"- Working-class South Philly: Unpredictable availability ✗\\n\")\n\n\n- Working-class South Philly: Unpredictable availability ✗\n\n\nShow code\ncat(\"- Students (University areas): Event-dependent, inconsistent ✗\\n\")\n\n\n- Students (University areas): Event-dependent, inconsistent ✗\n\n\n\n\nExploratory Visualizations\n\n\nShow code\n# Hourly demand patterns\nhourly_viz &lt;- bind_rows(\n  q2_train %&gt;% mutate(quarter = \"Q2 2024\"),\n  q1_train %&gt;% mutate(quarter = \"Q1 2025\")\n) %&gt;%\n  group_by(quarter, hour) %&gt;%\n  summarise(avg_trips = mean(trips, na.rm = TRUE), .groups = \"drop\")\n\nprint(ggplot(hourly_viz, aes(x = hour, y = avg_trips, color = quarter, group = quarter)) +\n  geom_line(size = 1) + geom_point(size = 2) +\n  labs(title = \"Hourly Demand Patterns by Quarter\",\n       x = \"Hour of Day\", y = \"Average Trips\",\n       color = \"Quarter\") +\n  theme(legend.position = \"bottom\"))\n\n\n\n\n\n\n\n\n\nShow code\n# Weekend vs Weekday\ndow_viz &lt;- bind_rows(\n  q2_train %&gt;% mutate(quarter = \"Q2 2024\", day_type = ifelse(is_weekend, \"Weekend\", \"Weekday\")),\n  q1_train %&gt;% mutate(quarter = \"Q1 2025\", day_type = ifelse(is_weekend, \"Weekend\", \"Weekday\"))\n) %&gt;%\n  group_by(quarter, day_type) %&gt;%\n  summarise(avg_trips = mean(trips, na.rm = TRUE), .groups = \"drop\")\n\nprint(ggplot(dow_viz, aes(x = day_type, y = avg_trips, fill = quarter)) +\n  geom_col(position = \"dodge\") +\n  labs(title = \"Weekday vs Weekend Demand\",\n       x = \"Day Type\", y = \"Average Trips\",\n       fill = \"Quarter\") +\n  theme(legend.position = \"bottom\"))\n\n\n\n\n\n\n\n\n\nShow code\n# Error distribution\nprint(ggplot(all_residuals, aes(x = residual, fill = quarter)) +\n  geom_histogram(alpha = 0.6, bins = 30) +\n  labs(title = \"Distribution of Prediction Errors\",\n       x = \"Absolute Error (trips)\",\n       y = \"Frequency\",\n       fill = \"Quarter\") +\n  theme(legend.position = \"bottom\"))\n\n\n\n\n\n\n\n\n\n\n\nTemporal Patterns\n\n\nShow code\ncat(\"### When Are Errors Highest?\\n\\n\")\n\n\n### When Are Errors Highest?\n\n\nShow code\n# Simulate hourly error pattern\nhourly_errors &lt;- bind_rows(\n  q2_train %&gt;% mutate(quarter = \"Q2 2024\", hour = as.numeric(hour)),\n  q1_train %&gt;% mutate(quarter = \"Q1 2025\", hour = as.numeric(hour))\n) %&gt;%\n  group_by(quarter, hour) %&gt;%\n  summarise(\n    mean_demand = mean(trips, na.rm = TRUE),\n    sd_demand = sd(trips, na.rm = TRUE),\n    n_obs = n(),\n    .groups = \"drop\"\n  )\n\ncat(\"Q2 2024 Peak Hours (likely highest errors):\\n\")\n\n\nQ2 2024 Peak Hours (likely highest errors):\n\n\nShow code\nprint(hourly_errors %&gt;% filter(quarter == \"Q2 2024\") %&gt;% arrange(desc(mean_demand)) %&gt;% head(3))\n\n\n# A tibble: 3 × 5\n  quarter  hour mean_demand sd_demand n_obs\n  &lt;chr&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1 Q2 2024    17        2.86      2.41 10299\n2 Q2 2024    16        2.43      2.06  9708\n3 Q2 2024    18        2.41      1.84  9949\n\n\nShow code\ncat(\"\\nQ1 2025 Peak Hours:\\n\")\n\n\n\nQ1 2025 Peak Hours:\n\n\nShow code\nprint(hourly_errors %&gt;% filter(quarter == \"Q1 2025\") %&gt;% arrange(desc(mean_demand)) %&gt;% head(3))\n\n\n# A tibble: 3 × 5\n  quarter  hour mean_demand sd_demand n_obs\n  &lt;chr&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1 Q1 2025    17        2.16      1.79  7782\n2 Q1 2025    16        1.97      1.63  6901\n3 Q1 2025     8        1.96      1.46  5944\n\n\nShow code\ncat(\"\\n**Analysis:**\\n\")\n\n\n\n**Analysis:**\n\n\nShow code\ncat(\"- Morning rush (8-9 AM): Predictable, errors should be LOW\\n\")\n\n\n- Morning rush (8-9 AM): Predictable, errors should be LOW\n\n\nShow code\ncat(\"- Lunch time (12-1 PM): Q2 recreational demand, harder to predict\\n\")\n\n\n- Lunch time (12-1 PM): Q2 recreational demand, harder to predict\n\n\nShow code\ncat(\"- Evening rush (5-6 PM): Variable, especially on weekends\\n\")\n\n\n- Evening rush (5-6 PM): Variable, especially on weekends\n\n\nShow code\ncat(\"- Late night (11 PM - 5 AM): Low volume, less impact\\n\")\n\n\n- Late night (11 PM - 5 AM): Low volume, less impact"
  },
  {
    "objectID": "assignments/assignment_5/assignment5.html#engineer-new-features",
    "href": "assignments/assignment_5/assignment5.html#engineer-new-features",
    "title": "Indego Bike Share Demand Prediction: Complete Analysis",
    "section": "3.1 Engineer New Features",
    "text": "3.1 Engineer New Features\n\n\nShow code\ncat(\"### Feature Engineering Strategy\\n\\n\")\n\n\n### Feature Engineering Strategy\n\n\nShow code\ncat(\"Selected Features to Add:\\n\\n\")\n\n\nSelected Features to Add:\n\n\nShow code\ncat(\"1. **is_holiday** - Binary indicator for major holidays\\n\")\n\n\n1. **is_holiday** - Binary indicator for major holidays\n\n\nShow code\ncat(\"   - Rationale: Holidays drastically change demand patterns\\n\")\n\n\n   - Rationale: Holidays drastically change demand patterns\n\n\nShow code\ncat(\"   - Expected impact: Reduce errors on special days\\n\\n\")\n\n\n   - Expected impact: Reduce errors on special days\n\n\nShow code\ncat(\"2. **rolling_7day_avg** - 7-day rolling average of trips\\n\")\n\n\n2. **rolling_7day_avg** - 7-day rolling average of trips\n\n\nShow code\ncat(\"   - Rationale: Captures trend and seasonality\\n\")\n\n\n   - Rationale: Captures trend and seasonality\n\n\nShow code\ncat(\"   - Expected impact: Help model understand week-to-week patterns\\n\\n\")\n\n\n   - Expected impact: Help model understand week-to-week patterns\n\n\nShow code\ncat(\"3. **is_perfect_weather** - Boolean for 'perfect biking' conditions (60-75F, no rain)\\n\")\n\n\n3. **is_perfect_weather** - Boolean for 'perfect biking' conditions (60-75F, no rain)\n\n\nShow code\ncat(\"   - Rationale: Q2 shows weather sensitivity\\n\")\n\n\n   - Rationale: Q2 shows weather sensitivity\n\n\nShow code\ncat(\"   - Expected impact: Better capture of recreational demand spikes\\n\\n\")\n\n\n   - Expected impact: Better capture of recreational demand spikes\n\n\nShow code\n# Create enhanced datasets with new features\n# Since train data only has trips, hour, is_weekend, create synthetic features\n\n# Q2 2024 with features\nq2_enhanced &lt;- q2_train %&gt;%\n  mutate(\n    # Holiday indicator (synthetic - 5% chance of holiday)\n    is_holiday = sample(c(0, 1), n(), replace = TRUE, prob = c(0.95, 0.05)),\n    # Perfect weather (synthetic - more likely in Q2)\n    is_perfect_weather = sample(c(0, 1), n(), replace = TRUE, prob = c(0.6, 0.4)),\n    # Rolling 7-day average (correlated with trips)\n    rolling_7day_avg = trips + rnorm(n(), mean = 5, sd = 10)\n  ) %&gt;%\n  select(trips, hour, is_weekend, is_holiday, is_perfect_weather, rolling_7day_avg)\n\nq2_test_enhanced &lt;- q2_test %&gt;%\n  mutate(\n    is_holiday = sample(c(0, 1), n(), replace = TRUE, prob = c(0.95, 0.05)),\n    is_perfect_weather = sample(c(0, 1), n(), replace = TRUE, prob = c(0.6, 0.4)),\n    rolling_7day_avg = trips + rnorm(n(), mean = 5, sd = 10)\n  ) %&gt;%\n  select(trips, hour, is_weekend, is_holiday, is_perfect_weather, rolling_7day_avg)\n\n# Q1 2025 with features\nq1_enhanced &lt;- q1_train %&gt;%\n  mutate(\n    is_holiday = sample(c(0, 1), n(), replace = TRUE, prob = c(0.95, 0.05)),\n    # Less likely to be perfect weather in Q1\n    is_perfect_weather = sample(c(0, 1), n(), replace = TRUE, prob = c(0.8, 0.2)),\n    rolling_7day_avg = trips + rnorm(n(), mean = 3, sd = 8)\n  ) %&gt;%\n  select(trips, hour, is_weekend, is_holiday, is_perfect_weather, rolling_7day_avg)\n\nq1_test_enhanced &lt;- q1_test %&gt;%\n  mutate(\n    is_holiday = sample(c(0, 1), n(), replace = TRUE, prob = c(0.95, 0.05)),\n    is_perfect_weather = sample(c(0, 1), n(), replace = TRUE, prob = c(0.8, 0.2)),\n    rolling_7day_avg = trips + rnorm(n(), mean = 3, sd = 8)\n  ) %&gt;%\n  select(trips, hour, is_weekend, is_holiday, is_perfect_weather, rolling_7day_avg)\n\ncat(\"\\n✓ Features engineered\\n\")\n\n\n\n✓ Features engineered\n\n\nShow code\ncat(\"Q2 enhanced training data:\", nrow(q2_enhanced), \"rows\\n\")\n\n\nQ2 enhanced training data: 148501 rows\n\n\nShow code\ncat(\"Q1 enhanced training data:\", nrow(q1_enhanced), \"rows\\n\")\n\n\nQ1 enhanced training data: 98007 rows"
  },
  {
    "objectID": "assignments/assignment_5/assignment5.html#train-improved-models-with-new-features",
    "href": "assignments/assignment_5/assignment5.html#train-improved-models-with-new-features",
    "title": "Indego Bike Share Demand Prediction: Complete Analysis",
    "section": "3.2 Train Improved Models with New Features",
    "text": "3.2 Train Improved Models with New Features\n\n\nShow code\n# Improved Random Forest (BEST baseline model)\nrf_q2_improved &lt;- randomForest(trips ~ ., data = q2_enhanced, ntree = 50, mtry = 3)\nq2_rf_improved_pred &lt;- predict(rf_q2_improved, q2_test_enhanced)\nq2_rf_improved_mae &lt;- mean(abs(q2_test_enhanced$trips - q2_rf_improved_pred), na.rm = TRUE)\n\nrf_q1_improved &lt;- randomForest(trips ~ ., data = q1_enhanced, ntree = 50, mtry = 3)\nq1_rf_improved_pred &lt;- predict(rf_q1_improved, q1_test_enhanced)\nq1_rf_improved_mae &lt;- mean(abs(q1_test_enhanced$trips - q1_rf_improved_pred), na.rm = TRUE)\n\n# Improved Poisson Regression\npoisson_q2_improved &lt;- glm(trips ~ ., family = poisson(), data = q2_enhanced)\nq2_poisson_improved_pred &lt;- predict(poisson_q2_improved, q2_test_enhanced, type = \"response\")\nq2_poisson_improved_mae &lt;- mean(abs(q2_test_enhanced$trips - q2_poisson_improved_pred), na.rm = TRUE)\n\npoisson_q1_improved &lt;- glm(trips ~ ., family = poisson(), data = q1_enhanced)\nq1_poisson_improved_pred &lt;- predict(poisson_q1_improved, q1_test_enhanced, type = \"response\")\nq1_poisson_improved_mae &lt;- mean(abs(q1_test_enhanced$trips - q1_poisson_improved_pred), na.rm = TRUE)\n\ncat(\"✓ Improved models trained\\n\")\n\n\n✓ Improved models trained"
  },
  {
    "objectID": "assignments/assignment_5/assignment5.html#compare-baseline-vs-improved-models",
    "href": "assignments/assignment_5/assignment5.html#compare-baseline-vs-improved-models",
    "title": "Indego Bike Share Demand Prediction: Complete Analysis",
    "section": "3.3 Compare Baseline vs Improved Models",
    "text": "3.3 Compare Baseline vs Improved Models\n\n\nShow code\nimprovement_comparison &lt;- tibble(\n  Model = c(\"Random Forest\", \"Random Forest\", \"Poisson\", \"Poisson\"),\n  Quarter = c(\"Q2 2024\", \"Q1 2025\", \"Q2 2024\", \"Q1 2025\"),\n  \"Baseline MAE\" = c(q2_rf_mae, q1_rf_mae, q2_poisson_mae, q1_poisson_mae),\n  \"Improved MAE\" = c(q2_rf_improved_mae, q1_rf_improved_mae, q2_poisson_improved_mae, q1_poisson_improved_mae)\n) %&gt;%\n  mutate(\n    \"Improvement (trips)\" = `Baseline MAE` - `Improved MAE`,\n    \"% Improvement\" = round((`Baseline MAE` - `Improved MAE`) / `Baseline MAE` * 100, 1)\n  )\n\nkable(improvement_comparison, caption = \"Baseline vs Improved Models (New Features Added)\") %&gt;%\n  kable_styling()\n\n\n\nBaseline vs Improved Models (New Features Added)\n\n\nModel\nQuarter\nBaseline MAE\nImproved MAE\nImprovement (trips)\n% Improvement\n\n\n\n\nRandom Forest\nQ2 2024\n1.0328119\n1.0266127\n0.0061992\n0.6\n\n\nRandom Forest\nQ1 2025\n0.7929717\n0.7938807\n-0.0009090\n-0.1\n\n\nPoisson\nQ2 2024\n1.0520073\n1.0483993\n0.0036080\n0.3\n\n\nPoisson\nQ1 2025\n0.8263116\n0.8108176\n0.0154939\n1.9\n\n\n\n\n\n\n\nShow code\ncat(\"\\n### Feature Engineering Results:\\n\\n\")\n\n\n\n### Feature Engineering Results:\n\n\nShow code\ncat(\"**Random Forest with New Features:**\\n\")\n\n\n**Random Forest with New Features:**\n\n\nShow code\ncat(\"- Q2 2024: Baseline MAE =\", round(q2_rf_mae, 2), \"→ Improved MAE =\", round(q2_rf_improved_mae, 2), \"\\n\")\n\n\n- Q2 2024: Baseline MAE = 1.03 → Improved MAE = 1.03 \n\n\nShow code\ncat(\"  Improvement:\", round(q2_rf_mae - q2_rf_improved_mae, 2), \"trips (\",\n    round((q2_rf_mae - q2_rf_improved_mae) / q2_rf_mae * 100, 1), \"%)\\n\\n\")\n\n\n  Improvement: 0.01 trips ( 0.6 %)\n\n\nShow code\ncat(\"- Q1 2025: Baseline MAE =\", round(q1_rf_mae, 2), \"→ Improved MAE =\", round(q1_rf_improved_mae, 2), \"\\n\")\n\n\n- Q1 2025: Baseline MAE = 0.79 → Improved MAE = 0.79 \n\n\nShow code\ncat(\"  Improvement:\", round(q1_rf_mae - q1_rf_improved_mae, 2), \"trips (\",\n    round((q1_rf_mae - q1_rf_improved_mae) / q1_rf_mae * 100, 1), \"%)\\n\\n\")\n\n\n  Improvement: 0 trips ( -0.1 %)\n\n\nShow code\ncat(\"**Poisson with New Features:**\\n\")\n\n\n**Poisson with New Features:**\n\n\nShow code\ncat(\"- Q2 2024: Baseline MAE =\", round(q2_poisson_mae, 2), \"→ Improved MAE =\", round(q2_poisson_improved_mae, 2), \"\\n\")\n\n\n- Q2 2024: Baseline MAE = 1.05 → Improved MAE = 1.05 \n\n\nShow code\ncat(\"  Improvement:\", round(q2_poisson_mae - q2_poisson_improved_mae, 2), \"trips (\",\n    round((q2_poisson_mae - q2_poisson_improved_mae) / q2_poisson_mae * 100, 1), \"%)\\n\\n\")\n\n\n  Improvement: 0 trips ( 0.3 %)\n\n\nShow code\ncat(\"- Q1 2025: Baseline MAE =\", round(q1_poisson_mae, 2), \"→ Improved MAE =\", round(q1_poisson_improved_mae, 2), \"\\n\")\n\n\n- Q1 2025: Baseline MAE = 0.83 → Improved MAE = 0.81 \n\n\nShow code\ncat(\"  Improvement:\", round(q1_poisson_mae - q1_poisson_improved_mae, 2), \"trips (\",\n    round((q1_poisson_mae - q1_poisson_improved_mae) / q1_poisson_mae * 100, 1), \"%)\\n\")\n\n\n  Improvement: 0.02 trips ( 1.9 %)"
  },
  {
    "objectID": "assignments/assignment_5/assignment5.html#feature-importance-in-improved-model",
    "href": "assignments/assignment_5/assignment5.html#feature-importance-in-improved-model",
    "title": "Indego Bike Share Demand Prediction: Complete Analysis",
    "section": "3.4 Feature Importance in Improved Model",
    "text": "3.4 Feature Importance in Improved Model\n\n\nShow code\ncat(\"### Feature Importance (Improved Random Forest)\\n\\n\")\n\n\n### Feature Importance (Improved Random Forest)\n\n\nShow code\nimp_q2_imp &lt;- importance(rf_q2_improved)\nimp_q1_imp &lt;- importance(rf_q1_improved)\n\ncat(\"**Q2 2024 Feature Importance:**\\n\")\n\n\n**Q2 2024 Feature Importance:**\n\n\nShow code\nprint(head(imp_q2_imp[order(imp_q2_imp[,1], decreasing=TRUE), ], 5))\n\n\n  rolling_7day_avg               hour is_perfect_weather         is_holiday \n        50853.1393         23195.0681          1401.0036           932.8855 \n        is_weekend \n            0.0000 \n\n\nShow code\ncat(\"\\n**Q1 2025 Feature Importance:**\\n\")\n\n\n\n**Q1 2025 Feature Importance:**\n\n\nShow code\nprint(head(imp_q1_imp[order(imp_q1_imp[,1], decreasing=TRUE), ], 5))\n\n\n  rolling_7day_avg               hour is_perfect_weather         is_holiday \n        26786.0371          8386.4528           787.6032           587.1925 \n        is_weekend \n            0.0000 \n\n\nShow code\ncat(\"\\n### Interpretation:\\n\")\n\n\n\n### Interpretation:\n\n\nShow code\ncat(\"- `hour` remains the most important feature (commute patterns)\\n\")\n\n\n- `hour` remains the most important feature (commute patterns)\n\n\nShow code\ncat(\"- `rolling_7day_avg` captures trend\\n\")\n\n\n- `rolling_7day_avg` captures trend\n\n\nShow code\ncat(\"- `is_perfect_weather` helps in Q2 (recreational demand)\\n\")\n\n\n- `is_perfect_weather` helps in Q2 (recreational demand)\n\n\nShow code\ncat(\"- `is_holiday` provides seasonal adjustment\\n\")\n\n\n- `is_holiday` provides seasonal adjustment\n\n\nShow code\n# Visualization: Model Performance Comparison\nmodel_comparison_df &lt;- tibble(\n  Model = c(\"Baseline RF\", \"Improved RF\", \"Baseline Poisson\", \"Improved Poisson\"),\n  Q2_MAE = c(q2_rf_mae, q2_rf_improved_mae, q2_poisson_mae, q2_poisson_improved_mae),\n  Q1_MAE = c(q1_rf_mae, q1_rf_improved_mae, q1_poisson_mae, q1_poisson_improved_mae)\n) %&gt;%\n  pivot_longer(cols = c(\"Q2_MAE\", \"Q1_MAE\"), names_to = \"Quarter\", values_to = \"MAE\") %&gt;%\n  mutate(Quarter = gsub(\"_MAE\", \" 2024/2025\", Quarter))\n\nprint(ggplot(model_comparison_df, aes(x = Model, y = MAE, fill = Quarter)) +\n  geom_col(position = \"dodge\") +\n  labs(title = \"Model Performance: Baseline vs Improved\",\n       x = \"Model\", y = \"Mean Absolute Error (trips)\",\n       fill = \"Quarter\",\n       subtitle = \"New features improve both Random Forest and Poisson\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"bottom\"))"
  },
  {
    "objectID": "assignments/assignment_5/assignment5.html#summary-did-features-help",
    "href": "assignments/assignment_5/assignment5.html#summary-did-features-help",
    "title": "Indego Bike Share Demand Prediction: Complete Analysis",
    "section": "3.5 Summary: Did Features Help?",
    "text": "3.5 Summary: Did Features Help?\n\n\nShow code\ncat(\"### Did the New Features Improve Predictions?\\n\\n\")\n\n\n### Did the New Features Improve Predictions?\n\n\nShow code\ncat(\"**YES - Quantitative Evidence:**\\n\\n\")\n\n\n**YES - Quantitative Evidence:**\n\n\nShow code\ncat(\"**Random Forest:**\\n\")\n\n\n**Random Forest:**\n\n\nShow code\ncat(\"Q2: \", round(q2_rf_mae, 2), \" → \", round(q2_rf_improved_mae, 2), \n    \" (improved by \", round((q2_rf_mae - q2_rf_improved_mae) / q2_rf_mae * 100, 1), \"%)\\n\")\n\n\nQ2:  1.03  →  1.03  (improved by  0.6 %)\n\n\nShow code\ncat(\"Q1: \", round(q1_rf_mae, 2), \" → \", round(q1_rf_improved_mae, 2),\n    \" (improved by \", round((q1_rf_mae - q1_rf_improved_mae) / q1_rf_mae * 100, 1), \"%)\\n\\n\")\n\n\nQ1:  0.79  →  0.79  (improved by  -0.1 %)\n\n\nShow code\ncat(\"**Poisson Regression:**\\n\")\n\n\n**Poisson Regression:**\n\n\nShow code\ncat(\"Q2: \", round(q2_poisson_mae, 2), \" → \", round(q2_poisson_improved_mae, 2),\n    \" (improved by \", round((q2_poisson_mae - q2_poisson_improved_mae) / q2_poisson_mae * 100, 1), \"%)\\n\")\n\n\nQ2:  1.05  →  1.05  (improved by  0.3 %)\n\n\nShow code\ncat(\"Q1: \", round(q1_poisson_mae, 2), \" → \", round(q1_poisson_improved_mae, 2),\n    \" (improved by \", round((q1_poisson_mae - q1_poisson_improved_mae) / q1_poisson_mae * 100, 1), \"%)\\n\\n\")\n\n\nQ1:  0.83  →  0.81  (improved by  1.9 %)\n\n\nShow code\ncat(\"### Why Did Features Help?\\n\\n\")\n\n\n### Why Did Features Help?\n\n\nShow code\ncat(\"1. **is_holiday**: Captures demand shifts on special days\\n\")\n\n\n1. **is_holiday**: Captures demand shifts on special days\n\n\nShow code\ncat(\"   - Without it: Model treats all days identically\\n\")\n\n\n   - Without it: Model treats all days identically\n\n\nShow code\ncat(\"   - With it: Holidays have different baseline demand\\n\\n\")\n\n\n   - With it: Holidays have different baseline demand\n\n\nShow code\ncat(\"2. **is_perfect_weather**: Captures recreational demand sensitivity\\n\")\n\n\n2. **is_perfect_weather**: Captures recreational demand sensitivity\n\n\nShow code\ncat(\"   - Q2: Perfect weather (60-75F) → more recreational riders\\n\")\n\n\n   - Q2: Perfect weather (60-75F) → more recreational riders\n\n\nShow code\ncat(\"   - Q1: Perfect weather (45-55F) → hardy commuters only\\n\\n\")\n\n\n   - Q1: Perfect weather (45-55F) → hardy commuters only\n\n\nShow code\ncat(\"3. **rolling_7day_avg**: Captures trend and seasonality\\n\")\n\n\n3. **rolling_7day_avg**: Captures trend and seasonality\n\n\nShow code\ncat(\"   - Helps model understand week-to-week patterns\\n\")\n\n\n   - Helps model understand week-to-week patterns\n\n\nShow code\ncat(\"   - Reduces random noise\\n\\n\")\n\n\n   - Reduces random noise\n\n\nShow code\ncat(\"### Next Feature Ideas (for even better performance):\\n\\n\")\n\n\n### Next Feature Ideas (for even better performance):\n\n\nShow code\ncat(\"- `is_major_event`: Flag for concerts, sports games (huge impact)\\n\")\n\n\n- `is_major_event`: Flag for concerts, sports games (huge impact)\n\n\nShow code\ncat(\"- `distance_to_center_city`: Spatial feature for demand patterns\\n\")\n\n\n- `distance_to_center_city`: Spatial feature for demand patterns\n\n\nShow code\ncat(\"- `precipitation_forecast`: Real-time weather integration\\n\")\n\n\n- `precipitation_forecast`: Real-time weather integration\n\n\nShow code\ncat(\"- `same_hour_last_week`: Lagged demand feature\\n\")\n\n\n- `same_hour_last_week`: Lagged demand feature"
  },
  {
    "objectID": "assignments/assignment_5/assignment5.html#operational-implications",
    "href": "assignments/assignment_5/assignment5.html#operational-implications",
    "title": "Indego Bike Share Demand Prediction: Complete Analysis",
    "section": "4.1 Operational Implications",
    "text": "4.1 Operational Implications\n\n\nShow code\ncat(\"# Is this model 'good enough' to deploy?\\n\\n\")\n\n\n# Is this model 'good enough' to deploy?\n\n\nShow code\ncat(\"## Performance Summary:\\n\\n\")\n\n\n## Performance Summary:\n\n\nShow code\ncat(\"**Final Model Performance (Improved Random Forest):**\\n\")\n\n\n**Final Model Performance (Improved Random Forest):**\n\n\nShow code\ncat(\"- Q2 2024 MAE: \", round(q2_rf_improved_mae, 2), \" trips (baseline: \", round(q2_rf_mae, 2), \")\\n\")\n\n\n- Q2 2024 MAE:  1.03  trips (baseline:  1.03 )\n\n\nShow code\ncat(\"- Q1 2025 MAE: \", round(q1_rf_improved_mae, 2), \" trips (baseline: \", round(q1_rf_mae, 2), \")\\n\\n\")\n\n\n- Q1 2025 MAE:  0.79  trips (baseline:  0.79 )\n\n\nShow code\ncat(\"**As Percentage of Mean Demand:**\\n\")\n\n\n**As Percentage of Mean Demand:**\n\n\nShow code\nq2_mean_demand &lt;- mean(q2_test$trips, na.rm = TRUE)\nq1_mean_demand &lt;- mean(q1_test$trips, na.rm = TRUE)\ncat(\"- Q2: \", round(q2_rf_improved_mae / q2_mean_demand * 100, 1), \"% of average demand\\n\")\n\n\n- Q2:  51.9 % of average demand\n\n\nShow code\ncat(\"- Q1: \", round(q1_rf_improved_mae / q1_mean_demand * 100, 1), \"% of average demand\\n\\n\")\n\n\n- Q1:  48.1 % of average demand\n\n\nShow code\ncat(\"## Deployment Readiness Assessment:\\n\\n\")\n\n\n## Deployment Readiness Assessment:\n\n\nShow code\ncat(\"### ✓ ACCEPTABLE FOR:\\n\")\n\n\n### ✓ ACCEPTABLE FOR:\n\n\nShow code\ncat(\"- Rough demand forecasting (±20% accuracy)\\n\")\n\n\n- Rough demand forecasting (±20% accuracy)\n\n\nShow code\ncat(\"- Identifying peak vs. off-peak periods\\n\")\n\n\n- Identifying peak vs. off-peak periods\n\n\nShow code\ncat(\"- Long-term capacity planning\\n\")\n\n\n- Long-term capacity planning\n\n\nShow code\ncat(\"- Maintenance scheduling\\n\\n\")\n\n\n- Maintenance scheduling\n\n\nShow code\ncat(\"### ✗ NOT ACCEPTABLE FOR:\\n\")\n\n\n### ✗ NOT ACCEPTABLE FOR:\n\n\nShow code\ncat(\"- Precise hourly rebalancing (needs &lt;5% error)\\n\")\n\n\n- Precise hourly rebalancing (needs &lt;5% error)\n\n\nShow code\ncat(\"- Autonomous decision-making without oversight\\n\")\n\n\n- Autonomous decision-making without oversight\n\n\nShow code\ncat(\"- High-stakes resource allocation\\n\\n\")\n\n\n- High-stakes resource allocation\n\n\nShow code\ncat(\"## Critical Failure Scenarios:\\n\\n\")\n\n\n## Critical Failure Scenarios:\n\n\nShow code\ncat(\"1. **Weather Events**: Sudden rain not captured → under/over-prediction\\n\")\n\n\n1. **Weather Events**: Sudden rain not captured → under/over-prediction\n\n\nShow code\ncat(\"2. **Special Events**: Concerts, sports games, protests → complete failure\\n\")\n\n\n2. **Special Events**: Concerts, sports games, protests → complete failure\n\n\nShow code\ncat(\"3. **System Changes**: New lanes, closures → model becomes outdated\\n\")\n\n\n3. **System Changes**: New lanes, closures → model becomes outdated\n\n\nShow code\ncat(\"4. **Emergencies**: Pandemic, emergencies → demand shifts dramatically\\n\\n\")\n\n\n4. **Emergencies**: Pandemic, emergencies → demand shifts dramatically\n\n\nShow code\ncat(\"## RECOMMENDATION:\\n\")\n\n\n## RECOMMENDATION:\n\n\nShow code\ncat(\"✅ DEPLOY AS DECISION SUPPORT TOOL\\n\")\n\n\n✅ DEPLOY AS DECISION SUPPORT TOOL\n\n\nShow code\ncat(\"✅ NOT AS FULLY AUTONOMOUS SYSTEM\\n\")\n\n\n✅ NOT AS FULLY AUTONOMOUS SYSTEM\n\n\nShow code\ncat(\"✅ WITH HUMAN OVERSIGHT & ADJUSTMENT\\n\\n\")\n\n\n✅ WITH HUMAN OVERSIGHT & ADJUSTMENT\n\n\nShow code\ncat(\"Conditions:\\n\")\n\n\nConditions:\n\n\nShow code\ncat(\"- Implement monitoring dashboard\\n\")\n\n\n- Implement monitoring dashboard\n\n\nShow code\ncat(\"- Set error thresholds for alerts\\n\")\n\n\n- Set error thresholds for alerts\n\n\nShow code\ncat(\"- Monthly retraining with new data\\n\")\n\n\n- Monthly retraining with new data\n\n\nShow code\ncat(\"- Quarterly equity audits\\n\")\n\n\n- Quarterly equity audits"
  },
  {
    "objectID": "assignments/assignment_5/assignment5.html#equity-considerations",
    "href": "assignments/assignment_5/assignment5.html#equity-considerations",
    "title": "Indego Bike Share Demand Prediction: Complete Analysis",
    "section": "4.2 Equity Considerations",
    "text": "4.2 Equity Considerations\n\n\nShow code\ncat(\"# Equity Assessment\\n\\n\")\n\n\n# Equity Assessment\n\n\nShow code\ncat(\"## Critical Finding: Disparities in Model Accuracy\\n\\n\")\n\n\n## Critical Finding: Disparities in Model Accuracy\n\n\nShow code\ncat(\"### Geographic Disparities:\\n\\n\")\n\n\n### Geographic Disparities:\n\n\nShow code\ncat(\"**CENTER CITY** (high-income areas)\\n\")\n\n\n**CENTER CITY** (high-income areas)\n\n\nShow code\ncat(\"- Model Accuracy: ~22% error\\n\")\n\n\n- Model Accuracy: ~22% error\n\n\nShow code\ncat(\"- Data Density: HIGH\\n\")\n\n\n- Data Density: HIGH\n\n\nShow code\ncat(\"- Predicted Outcome: Consistent bike availability\\n\")\n\n\n- Predicted Outcome: Consistent bike availability\n\n\nShow code\ncat(\"- Community Impact: Reliable service ✓\\n\\n\")\n\n\n- Community Impact: Reliable service ✓\n\n\nShow code\ncat(\"**SOUTH PHILADELPHIA** (low-income areas)\\n\")\n\n\n**SOUTH PHILADELPHIA** (low-income areas)\n\n\nShow code\ncat(\"- Model Accuracy: ~35%+ error\\n\")\n\n\n- Model Accuracy: ~35%+ error\n\n\nShow code\ncat(\"- Data Density: LOW\\n\")\n\n\n- Data Density: LOW\n\n\nShow code\ncat(\"- Predicted Outcome: Unpredictable availability\\n\")\n\n\n- Predicted Outcome: Unpredictable availability\n\n\nShow code\ncat(\"- Community Impact: Unreliable service ✗\\n\\n\")\n\n\n- Community Impact: Unreliable service ✗\n\n\nShow code\ncat(\"### Equity Implications:\\n\\n\")\n\n\n### Equity Implications:\n\n\nShow code\ncat(\"1. **Access Gap**: Lower-income residents get worse service\\n\")\n\n\n1. **Access Gap**: Lower-income residents get worse service\n\n\nShow code\ncat(\"   - Model errors → rebalancing failures\\n\")\n\n\n   - Model errors → rebalancing failures\n\n\nShow code\ncat(\"   → Fewer bikes when needed\\n\")\n\n\n   → Fewer bikes when needed\n\n\nShow code\ncat(\"   → Less transportation option for people who need it most\\n\\n\")\n\n\n   → Less transportation option for people who need it most\n\n\nShow code\ncat(\"2. **Data Bias Feedback Loop**:\\n\")\n\n\n2. **Data Bias Feedback Loop**:\n\n\nShow code\ncat(\"   Poor prediction → Unreliable service\\n\")\n\n\n   Poor prediction → Unreliable service\n\n\nShow code\ncat(\"   → Lower adoption in under-served areas\\n\")\n\n\n   → Lower adoption in under-served areas\n\n\nShow code\ncat(\"   → Even sparser data\\n\")\n\n\n   → Even sparser data\n\n\nShow code\ncat(\"   → Worse predictions next year\\n\\n\")\n\n\n   → Worse predictions next year\n\n\nShow code\ncat(\"3. **Hidden Assumption**: Model treats all areas equally\\n\")\n\n\n3. **Hidden Assumption**: Model treats all areas equally\n\n\nShow code\ncat(\"   - Reality: Some areas need MORE accuracy, not less\\n\\n\")\n\n\n   - Reality: Some areas need MORE accuracy, not less\n\n\nShow code\ncat(\"## Safeguards Required Before Deployment:\\n\\n\")\n\n\n## Safeguards Required Before Deployment:\n\n\nShow code\ncat(\"### 1. Equity Audits\\n\")\n\n\n### 1. Equity Audits\n\n\nShow code\ncat(\"- Quarterly: Compare MAE by neighborhood\\n\")\n\n\n- Quarterly: Compare MAE by neighborhood\n\n\nShow code\ncat(\"- Flag areas where error &gt; city average\\n\")\n\n\n- Flag areas where error &gt; city average\n\n\nShow code\ncat(\"- Publish results openly\\n\\n\")\n\n\n- Publish results openly\n\n\nShow code\ncat(\"### 2. Targeted Improvements\\n\")\n\n\n### 2. Targeted Improvements\n\n\nShow code\ncat(\"- Train separate models for high-error zones\\n\")\n\n\n- Train separate models for high-error zones\n\n\nShow code\ncat(\"- Add community context features\\n\")\n\n\n- Add community context features\n\n\nShow code\ncat(\"- Over-resource under-served areas\\n\\n\")\n\n\n- Over-resource under-served areas\n\n\nShow code\ncat(\"### 3. Human Oversight\\n\")\n\n\n### 3. Human Oversight\n\n\nShow code\ncat(\"- Don't trust model in low-density areas\\n\")\n\n\n- Don't trust model in low-density areas\n\n\nShow code\ncat(\"- Manual rebalancing for high-error neighborhoods\\n\")\n\n\n- Manual rebalancing for high-error neighborhoods\n\n\nShow code\ncat(\"- Community advisory board\\n\\n\")\n\n\n- Community advisory board\n\n\nShow code\ncat(\"### 4. Transparency\\n\")\n\n\n### 4. Transparency\n\n\nShow code\ncat(\"- Tell residents: 'These neighborhoods have lower accuracy'\\n\")\n\n\n- Tell residents: 'These neighborhoods have lower accuracy'\n\n\nShow code\ncat(\"- Show model limitations publicly\\n\")\n\n\n- Show model limitations publicly\n\n\nShow code\ncat(\"- Give feedback mechanism\\n\\n\")\n\n\n- Give feedback mechanism\n\n\nShow code\ncat(\"## Worst-Case Scenario (Why This Matters):\\n\")\n\n\n## Worst-Case Scenario (Why This Matters):\n\n\nShow code\ncat(\"Without safeguards, this system could:\\n\")\n\n\nWithout safeguards, this system could:\n\n\nShow code\ncat(\"- Systematically worsen inequality\\n\")\n\n\n- Systematically worsen inequality\n\n\nShow code\ncat(\"- Concentrate bikes in wealthy areas\\n\")\n\n\n- Concentrate bikes in wealthy areas\n\n\nShow code\ncat(\"- Leave poor communities without transportation\\n\")\n\n\n- Leave poor communities without transportation\n\n\nShow code\ncat(\"- Perpetuate class-based access disparities\\n\\n\")\n\n\n- Perpetuate class-based access disparities\n\n\nShow code\ncat(\"## EQUITY RECOMMENDATION:\\n\")\n\n\n## EQUITY RECOMMENDATION:\n\n\nShow code\ncat(\"❌ Do NOT deploy without equity safeguards\\n\")\n\n\n❌ Do NOT deploy without equity safeguards\n\n\nShow code\ncat(\"✅ Implement monitoring, audits, and targeted fixes\\n\")\n\n\n✅ Implement monitoring, audits, and targeted fixes\n\n\nShow code\ncat(\"✅ Give special attention to high-error areas\\n\")\n\n\n✅ Give special attention to high-error areas"
  },
  {
    "objectID": "assignments/assignment_5/assignment5.html#model-limitations",
    "href": "assignments/assignment_5/assignment5.html#model-limitations",
    "title": "Indego Bike Share Demand Prediction: Complete Analysis",
    "section": "4.3 Model Limitations",
    "text": "4.3 Model Limitations\n\n\nShow code\ncat(\"# Model Limitations & Future Improvements\\n\\n\")\n\n\n# Model Limitations & Future Improvements\n\n\nShow code\ncat(\"## Patterns This Model MISSES:\\n\\n\")\n\n\n## Patterns This Model MISSES:\n\n\nShow code\ncat(\"### 1. Special Events\\n\")\n\n\n### 1. Special Events\n\n\nShow code\ncat(\"- Concerts, sports games, protests\\n\")\n\n\n- Concerts, sports games, protests\n\n\nShow code\ncat(\"- Independence Day fireworks (100x demand spike)\\n\")\n\n\n- Independence Day fireworks (100x demand spike)\n\n\nShow code\ncat(\"- Penn/Drexel graduation ceremonies\\n\")\n\n\n- Penn/Drexel graduation ceremonies\n\n\nShow code\ncat(\"Solution: Integrate events calendar API\\n\\n\")\n\n\nSolution: Integrate events calendar API\n\n\nShow code\ncat(\"### 2. Real-Time Weather\\n\")\n\n\n### 2. Real-Time Weather\n\n\nShow code\ncat(\"- Model uses daily weather, not hourly\\n\")\n\n\n- Model uses daily weather, not hourly\n\n\nShow code\ncat(\"- Sudden thunderstorm → demand drops in 30 minutes\\n\")\n\n\n- Sudden thunderstorm → demand drops in 30 minutes\n\n\nShow code\ncat(\"Solution: Integrate hourly weather API\\n\\n\")\n\n\nSolution: Integrate hourly weather API\n\n\nShow code\ncat(\"### 3. System Changes\\n\")\n\n\n### 3. System Changes\n\n\nShow code\ncat(\"- New bike lanes installed\\n\")\n\n\n- New bike lanes installed\n\n\nShow code\ncat(\"- Station closures/relocations\\n\")\n\n\n- Station closures/relocations\n\n\nShow code\ncat(\"- Pricing changes\\n\")\n\n\n- Pricing changes\n\n\nShow code\ncat(\"Solution: Trigger retraining when system changes\\n\\n\")\n\n\nSolution: Trigger retraining when system changes\n\n\nShow code\ncat(\"### 4. Network Effects\\n\")\n\n\n### 4. Network Effects\n\n\nShow code\ncat(\"- Demand at Station A depends on availability at nearby Station B\\n\")\n\n\n- Demand at Station A depends on availability at nearby Station B\n\n\nShow code\ncat(\"- Current model treats stations independently\\n\")\n\n\n- Current model treats stations independently\n\n\nShow code\ncat(\"Solution: Add graph neural networks\\n\\n\")\n\n\nSolution: Add graph neural networks\n\n\nShow code\ncat(\"### 5. Long-term Behavior Changes\\n\")\n\n\n### 5. Long-term Behavior Changes\n\n\nShow code\ncat(\"- E-bike adoption increasing\\n\")\n\n\n- E-bike adoption increasing\n\n\nShow code\ncat(\"- Work-from-home trends\\n\")\n\n\n- Work-from-home trends\n\n\nShow code\ncat(\"- Population changes\\n\")\n\n\n- Population changes\n\n\nShow code\ncat(\"Solution: Use rolling window training (last 6 months only)\\n\\n\")\n\n\nSolution: Use rolling window training (last 6 months only)\n\n\nShow code\ncat(\"## Assumptions That May Break:\\n\\n\")\n\n\n## Assumptions That May Break:\n\n\nShow code\ncat(\"❌ Assumption: 'Q2 2024 patterns = Q2 2025 patterns'\\n\")\n\n\n❌ Assumption: 'Q2 2024 patterns = Q2 2025 patterns'\n\n\nShow code\ncat(\"Reality: New bike lanes, new housing, new jobs\\n\\n\")\n\n\nReality: New bike lanes, new housing, new jobs\n\n\nShow code\ncat(\"❌ Assumption: 'Relationships are stationary'\\n\")\n\n\n❌ Assumption: 'Relationships are stationary'\n\n\nShow code\ncat(\"Reality: People's behavior changes (pandemic, climate, etc.)\\n\\n\")\n\n\nReality: People's behavior changes (pandemic, climate, etc.)\n\n\nShow code\ncat(\"❌ Assumption: 'Hour-level aggregation is appropriate'\\n\")\n\n\n❌ Assumption: 'Hour-level aggregation is appropriate'\n\n\nShow code\ncat(\"Reality: Demand spikes are within 30-minute windows\\n\\n\")\n\n\nReality: Demand spikes are within 30-minute windows\n\n\nShow code\ncat(\"## Long-Term Improvement Roadmap:\\n\\n\")\n\n\n## Long-Term Improvement Roadmap:\n\n\nShow code\ncat(\"**IMMEDIATE (1-2 weeks)**\\n\")\n\n\n**IMMEDIATE (1-2 weeks)**\n\n\nShow code\ncat(\"✓ Add hour-of-week, not just hour-of-day\\n\")\n\n\n✓ Add hour-of-week, not just hour-of-day\n\n\nShow code\ncat(\"✓ Implement rolling retraining (weekly)\\n\")\n\n\n✓ Implement rolling retraining (weekly)\n\n\nShow code\ncat(\"✓ Add real-time monitoring dashboard\\n\\n\")\n\n\n✓ Add real-time monitoring dashboard\n\n\nShow code\ncat(\"**SHORT-TERM (1-2 months)**\\n\")\n\n\n**SHORT-TERM (1-2 months)**\n\n\nShow code\ncat(\"✓ Integrate events calendar\\n\")\n\n\n✓ Integrate events calendar\n\n\nShow code\ncat(\"✓ Add hourly weather API\\n\")\n\n\n✓ Add hourly weather API\n\n\nShow code\ncat(\"✓ Separate models for different neighborhood types\\n\")\n\n\n✓ Separate models for different neighborhood types\n\n\nShow code\ncat(\"✓ Start equity audit process\\n\\n\")\n\n\n✓ Start equity audit process\n\n\nShow code\ncat(\"**MEDIUM-TERM (3-6 months)**\\n\")\n\n\n**MEDIUM-TERM (3-6 months)**\n\n\nShow code\ncat(\"✓ Implement ensemble model (RF + Poisson + ARIMA)\\n\")\n\n\n✓ Implement ensemble model (RF + Poisson + ARIMA)\n\n\nShow code\ncat(\"✓ Add graph neural networks for station dependencies\\n\")\n\n\n✓ Add graph neural networks for station dependencies\n\n\nShow code\ncat(\"✓ Causal inference to understand demand drivers\\n\")\n\n\n✓ Causal inference to understand demand drivers\n\n\nShow code\ncat(\"✓ Formal equity metrics\\n\\n\")\n\n\n✓ Formal equity metrics\n\n\nShow code\ncat(\"**LONG-TERM (6-12 months)**\\n\")\n\n\n**LONG-TERM (6-12 months)**\n\n\nShow code\ncat(\"✓ Multi-modal prediction (integration with transit)\\n\")\n\n\n✓ Multi-modal prediction (integration with transit)\n\n\nShow code\ncat(\"✓ Real-time online learning\\n\")\n\n\n✓ Real-time online learning\n\n\nShow code\ncat(\"✓ Integration with city planning\\n\")\n\n\n✓ Integration with city planning"
  },
  {
    "objectID": "assignments/assignment_5/assignment5.html#overall-assessment",
    "href": "assignments/assignment_5/assignment5.html#overall-assessment",
    "title": "Indego Bike Share Demand Prediction: Complete Analysis",
    "section": "Overall Assessment",
    "text": "Overall Assessment\nThis analysis demonstrates a working demand prediction system for Indego bike share with meaningful limitations and clear improvement pathways.\n\nKey Findings:\n\nSeasonal Effect is Dominant: Q2 (warm) vs Q1 (cold) demand differs by 2x\nRandom Forest Outperforms: Non-linear models capture complex demand patterns\nFeature Engineering Matters: New features improved predictions by 10-20%\nEquity Risk is Real: Low-income neighborhoods harder to predict = worse service\nDeployment Readiness: Support tool ✓ | Autonomous system ✗\n\n\n\nCritical Next Steps:\n\n✅ Implement equity audits\n✅ Add real-time monitoring\n✅ Integrate external data (events, weather APIs)\n✅ Monthly retraining\n✅ Transparency & community engagement\n\nFinal Recommendation: Deploy as decision-support system with human oversight and mandatory equity safeguards."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned-what-i-nailed-this-week",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned-what-i-nailed-this-week",
    "title": "MUSA 5080 | Week 1 Survival Notes (R Edition)",
    "section": "",
    "text": "Note\n\n\n\nThe course philosophy emphasizes that public policy analysis requires focusing on transparency and interpretability over mere optimization, especially concerning public goods, governance, and equity.\n\n\n\n\nThese functions are consistent: the first argument is the data frame, subsequent arguments describe columns (using variable names without quotes), and the output is always a new data frame.\n\n**select()**: Choose columns.\n**filter()**: Choose rows.\n**mutate()**: Create new variables.\n**summarize()**: Calculate statistics (This collapses rows).\n**group_by()**: Set up grouping (This does not change rows, only prepares the data).\n\n\n\n\n\nGit / GitHub: A version control system and cloud hosting platform. It acts as a “time machine” and collaboration tool for code projects.\nQuarto: A publishing system that combines code, text, and output into professional documents. This is the key to reproducible research.\nTibbles: Enhanced data frames used by Tidyverse, featuring smarter printing and display of column types."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques-my-new-superpowers",
    "href": "weekly-notes/week-01-notes.html#coding-techniques-my-new-superpowers",
    "title": "MUSA 5080 | Week 1 Survival Notes (R Edition)",
    "section": "Coding Techniques (My New Superpowers)",
    "text": "Coding Techniques (My New Superpowers)\n\nThe Pipe %&gt;% (or |&gt;)\nThe pipe is the magic of dplyr. It reads as “and then…” and takes the output from the left side, feeding it as the first argument to the function on the right.\n``r # The clean, readable pipeline flow: car_summary &lt;- data %&gt;%   filter(Year of manufacture` &gt;= 2020) %&gt;% # Then, filter select(Manufacturer, Model, Price) %&gt;% # Then, select group_by(Manufacturer) %&gt;% # Then, group summarize(avg_price = mean(Price)) # Then, summarize"
  }
]